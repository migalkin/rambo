{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransE\n",
    "\n",
    "**Data**: s, p, o, qp, qe\n",
    "\n",
    "$s,p,o,qe,qp \\in R^{d}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "1. Model\n",
    "2. Sampling Code\n",
    "\n",
    "\n",
    "**Model** from [pyKeen](https://github.com/SmartDataAnalytics/PyKEEN/blob/master/src/pykeen/kge_models/trans_e.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "# from dataclasses import dataclass\n",
    "from typing import Dict\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pandas as  pd\n",
    "import numpy as np\n",
    "import traceback\n",
    "import warnings\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.autograd\n",
    "from torch import nn\n",
    "\n",
    "from functools import partial\n",
    "from typing import Optional, Union, List, Callable\n",
    "\n",
    "# MyTorch imports\n",
    "from mytorch.utils.goodies import *\n",
    "from mytorch import dataiters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Local imports\n",
    "# from raw_parser import *\n",
    "from utils import *\n",
    "from corruption import sample_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Stats\n",
    "n_entities, n_relations etc"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Load data from disk\n",
    "with open('./parsed_raw_data.pkl', 'rb') as f:\n",
    "    raw_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "entities, predicates = [], []\n",
    "\n",
    "for quint in raw_data:\n",
    "    entities += [quint[0], quint[2]]\n",
    "    if quint[4]:\n",
    "        entities.append(quint[4])\n",
    "        \n",
    "    predicates.append(quint[1])\n",
    "    if quint[3]: \n",
    "        predicates.append(quint[3])\n",
    "    \n",
    "entities = list(set(entities))\n",
    "predicates = list(set(predicates))\n",
    "        \n",
    "print(len(entities), len(predicates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransE Neural Model\n",
    "Code from PyKeen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_CONFIG = {\n",
    "    'NUM_ENTITIES': len(entities),\n",
    "    'NUM_RELATIONS': len(predicates),\n",
    "    'EMBEDDING_DIM': 200,\n",
    "    'NORM_FOR_NORMALIZATION_OF_ENTITIES': 2,\n",
    "    'NORM_FOR_NORMALIZATION_OF_RELATIONS': 2,\n",
    "    'SCORING_FUNCTION_NORM': 1,\n",
    "    'MARGIN_LOSS': 4,\n",
    "    'LEARNING_RATE': 0.001,\n",
    "    'NEGATIVE_SAMPLING_PROBS': [0.3, 0.0, 0.2, 0.5],\n",
    "    'NEGATIVE_SAMPLING_TIMES': 10,\n",
    "    'BATCH_SIZE': 256\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_triples(triples: torch.Tensor) -> List[torch.Tensor]:\n",
    "    \"\"\" Slice in 3 or 5 as needed \"\"\"\n",
    "    return triples[:,0], triples[:,1], triples[:,2], triples[:,3], triples[:, 4]\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Test slice_triples\n",
    "t = torch.randint(0, 100, (10, 5))\n",
    "t, slice_triples(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModule(nn.Module):\n",
    "    \"\"\"A base class for all of the models.\"\"\"\n",
    "\n",
    "    margin_ranking_loss_size_average: bool = None\n",
    "    entity_embedding_max_norm: Optional[int] = None\n",
    "    entity_embedding_norm_type: int = 2\n",
    "    hyper_params = [EXPERIMENT_CONFIG['EMBEDDING_DIM'], \n",
    "                    EXPERIMENT_CONFIG['MARGIN_LOSS'], \n",
    "                    EXPERIMENT_CONFIG['LEARNING_RATE']]\n",
    "\n",
    "    def __init__(self, config: Dict) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Device selection\n",
    "        self.device = config['DEVICE']\n",
    "\n",
    "        # Loss\n",
    "        self.margin_loss = config['MARGIN_LOSS']\n",
    "        self.criterion = nn.MarginRankingLoss(\n",
    "            margin=self.margin_loss,\n",
    "            reduction='mean' if self.margin_ranking_loss_size_average else 'sum'\n",
    "        )\n",
    "\n",
    "        # Entity dimensions\n",
    "        #: The number of entities in the knowledge graph\n",
    "        self.num_entities = config['NUM_ENTITIES']\n",
    "        #: The number of unique relation types in the knowledge graph\n",
    "        self.num_relations = config['NUM_RELATIONS']\n",
    "        #: The dimension of the embeddings to generate\n",
    "        self.embedding_dim = config['EMBEDDING_DIM']\n",
    "\n",
    "        self.entity_embeddings = nn.Embedding(\n",
    "            self.num_entities,\n",
    "            self.embedding_dim,\n",
    "            norm_type=self.entity_embedding_norm_type,\n",
    "            max_norm=self.entity_embedding_max_norm,\n",
    "        )\n",
    "\n",
    "    def __init_subclass__(cls, **kwargs):  # noqa: D105\n",
    "        if not getattr(cls, 'model_name', None):\n",
    "            raise TypeError('missing model_name class attribute')\n",
    "\n",
    "    def _get_entity_embeddings(self, entities):\n",
    "        return self.entity_embeddings(entities).view(-1, self.embedding_dim)\n",
    "\n",
    "    def _compute_loss(self, positive_scores: torch.Tensor, negative_scores: torch.Tensor) -> torch.Tensor:\n",
    "        y = np.repeat([-1], repeats=positive_scores.shape[0])\n",
    "        y = torch.tensor(y, dtype=torch.float, device=self.device)\n",
    "\n",
    "        loss = self.criterion(positive_scores, negative_scores, y)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransE(BaseModule):\n",
    "    \"\"\"An implementation of TransE [borders2013]_.\n",
    "     This model considers a relation as a translation from the head to the tail entity.\n",
    "    .. [borders2013] Bordes, A., *et al.* (2013). `Translating embeddings for modeling multi-relational data\n",
    "                     <http://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data.pdf>`_\n",
    "                     . NIPS.\n",
    "    .. seealso::\n",
    "       - Alternative implementation in OpenKE: https://github.com/thunlp/OpenKE/blob/OpenKE-PyTorch/models/TransE.py\n",
    "    \"\"\"\n",
    "\n",
    "    model_name = 'TransE MM'\n",
    "    margin_ranking_loss_size_average: bool = True\n",
    "    entity_embedding_max_norm: Optional[int] = None\n",
    "    entity_embedding_norm_type: int = 2\n",
    "    \n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__(config)\n",
    "\n",
    "        # Embeddings\n",
    "        self.l_p_norm_entities = config['NORM_FOR_NORMALIZATION_OF_ENTITIES']\n",
    "        self.scoring_fct_norm = config['SCORING_FUNCTION_NORM']\n",
    "        self.relation_embeddings = nn.Embedding(config['NUM_RELATIONS'], config['EMBEDDING_DIM'])\n",
    "        \n",
    "        self.config = config\n",
    "\n",
    "        self._initialize()\n",
    "\n",
    "    def _initialize(self):\n",
    "        embeddings_init_bound = 6 / np.sqrt(self.config['EMBEDDING_DIM'])\n",
    "        nn.init.uniform_(\n",
    "            self.entity_embeddings.weight.data,\n",
    "            a=-embeddings_init_bound,\n",
    "            b=+embeddings_init_bound,\n",
    "        )\n",
    "        nn.init.uniform_(\n",
    "            self.relation_embeddings.weight.data,\n",
    "            a=-embeddings_init_bound,\n",
    "            b=+embeddings_init_bound,\n",
    "        )\n",
    "\n",
    "        norms = torch.norm(self.relation_embeddings.weight, \n",
    "                           p=self.config['NORM_FOR_NORMALIZATION_OF_RELATIONS'], dim=1).data\n",
    "        self.relation_embeddings.weight.data = self.relation_embeddings.weight.data.div(\n",
    "            norms.view(self.num_relations, 1).expand_as(self.relation_embeddings.weight))\n",
    "\n",
    "    def predict(self, triples):\n",
    "        scores = self._score_triples(triples)\n",
    "        return scores.detach().cpu().numpy()\n",
    "\n",
    "    def forward(self, batch_positives, batch_negatives):\n",
    "        # Normalize embeddings of entities\n",
    "        norms = torch.norm(self.entity_embeddings.weight, p=self.l_p_norm_entities, dim=1).data\n",
    "        self.entity_embeddings.weight.data = self.entity_embeddings.weight.data.div(\n",
    "            norms.view(self.num_entities, 1).expand_as(self.entity_embeddings.weight))\n",
    "\n",
    "        positive_scores = self._score_triples(batch_positives)\n",
    "        negative_scores = self._score_triples(batch_negatives)\n",
    "        loss = self._compute_loss(positive_scores=positive_scores, negative_scores=negative_scores)\n",
    "        return loss\n",
    "\n",
    "    def _score_triples(self, triples):\n",
    "        \n",
    "        head_embeddings, relation_embeddings, tail_embeddings, qual_relation_embeddings, qual_entity_embeddings = self._get_triple_embeddings(triples)\n",
    "        scores = self._compute_scores(head_embeddings, relation_embeddings, tail_embeddings, qual_relation_embeddings, qual_entity_embeddings)\n",
    "        return scores\n",
    "\n",
    "    def _compute_scores(self, head_embeddings, relation_embeddings, tail_embeddings, qual_relation_embeddings, qual_entity_embeddings):\n",
    "        \"\"\"\n",
    "            Compute the scores based on the head, relation, and tail embeddings.\n",
    "        \n",
    "        :param head_embeddings: embeddings of head entities of dimension batchsize x embedding_dim\n",
    "        :param relation_embeddings: emebddings of relation embeddings of dimension batchsize x embedding_dim\n",
    "        :param tail_embeddings: embeddings of tail entities of dimension batchsize x embedding_dim\n",
    "        :param qual_relation_embeddings: embeddings of qualifier relation of dimension batchsize x embedding_dim\n",
    "        :param qual_entity_embeddings: embeddings of qualifier entity of dimension batchsize x embedding_dim\n",
    "        :return: Tensor of dimension batch_size containing the scores for each batch element\n",
    "        \"\"\"\n",
    "        # Add the vector element wise\n",
    "        sum_res = head_embeddings + relation_embeddings - tail_embeddings \\\n",
    "                                        + qual_relation_embeddings - qual_entity_embeddings\n",
    "        distances = torch.norm(sum_res, dim=1, p=self.scoring_fct_norm).view(size=(-1,))\n",
    "        return distances\n",
    "\n",
    "    def _get_triple_embeddings(self, triples):\n",
    "        heads, relations, tails, qual_relations, qual_entities = slice_triples(triples)\n",
    "        return (\n",
    "            self._get_entity_embeddings(heads),\n",
    "            self._get_relation_embeddings(relations),\n",
    "            self._get_entity_embeddings(tails),\n",
    "            self._get_relation_embeddings(qual_relations),\n",
    "            self._get_entity_embeddings(qual_entities)\n",
    "        )\n",
    "\n",
    "    def _get_relation_embeddings(self, relations):\n",
    "        return self.relation_embeddings(relations).view(-1, self.embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sampling etc\n",
    "\n",
    "TODO: Figuring out \n",
    "1. How to perturb data\n",
    "2. What ratios\n",
    "3. Train/Test splits\n",
    "4. Test task\n",
    "\n",
    "**CODE ORG**\n",
    "Instead of perturbing on the fly, we can pre-perturb it and then do splits and batches.\n",
    "\n",
    "**How to deal with cases w no qualifiers**\n",
    "    - treat them differently?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for datum in data:\n",
    "    case 1 (enter with p) -> fuck s\n",
    "    case 2 (enter with p) -> fuck o\n",
    "    case 3 (enter with p) -> fuck r\n",
    "    case 3 (enter with p)\n",
    "        -> either qe, qp exist\n",
    "            -> either you can remove qe, qp | or replace qe & qp | or replace qe | or replace qp\n",
    "        -> they dont exist\n",
    "            -> either you add qe qp\n",
    "    check if corrupted thing doesnt exist in dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Play with probs\n",
    "# [ p(s), p(r), p(o), p(q) ]\n",
    "# prob = [0.3, 0.0, 0.2, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Quint(s='Q636', p='P27', o='Q8854904', qp=None, qe=None),\n",
       " Quint(s='Q636', p='P27', o='Q145', qp=None, qe=None))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_negatives(raw_data[0], EXPERIMENT_CONFIG['NEGATIVE_SAMPLING_PROBS']), raw_data[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "negatives = [sample_negatives(datum, prob) for datum in raw_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_negatives(positive: List[Quint], probs: List[float], times: int):\n",
    "    \"\"\"\n",
    "        :param postive: List of the raw data\n",
    "        :param probs: List of probabilities to generate neg data following [ p(s), p(r), p(o), p(q) ]\n",
    "        :param times: how many negative samples per positive sample.\n",
    "    \"\"\"\n",
    "    negatives = []\n",
    "    for pos in tqdm(positive):\n",
    "        negatives_per_pos = [sample_negatives(pos, probs) for _ in range(times)]\n",
    "        negatives.append(negatives_per_pos)\n",
    "        \n",
    "    return negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    negatives = pickle.load(open(PRETRAINING_DATA_DIR / 'negatives.pkl', 'rb'))\n",
    "except (FileNotFoundError, IOError) as e:\n",
    "    # Generate it again\n",
    "    warnings.warn(\"Negative data not pre-generating. Takes three minutes.\")\n",
    "    negatives = generate_negatives(raw_data, \n",
    "                                   probs = EXPERIMENT_CONFIG['NEGATIVE_SAMPLING_PROBS'],\n",
    "                                   times = EXPERIMENT_CONFIG['NEGATIVE_SAMPLING_TIMES'])\n",
    "\n",
    "    # Dump this somewhere\n",
    "    with open(PRETRAINING_DATA_DIR / 'negatives.pkl', 'wb+') as f:\n",
    "        pickle.dump(negatives, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "_positives, _negatives = [], []\n",
    "for quint in raw_data:\n",
    "    _pos = [\n",
    "        entoid[quint[0]], \n",
    "        prtoid[quint[1]], \n",
    "        entoid[quint[2]], \n",
    "        prtoid[quint[3] if quint[3] else '__na__'], \n",
    "        entoid[quint[4] if quint[4] else '__na__']\n",
    "    ]\n",
    "    _positives.append(_pos)\n",
    "    \n",
    "for negative in negatives:\n",
    "    _negative = []\n",
    "    for quint in negative:\n",
    "        _neg = [\n",
    "            entoid[quint[0]], \n",
    "            prtoid[quint[1]], \n",
    "            entoid[quint[2]], \n",
    "            prtoid[quint[3] if quint[3] else '__na__'], \n",
    "            entoid[quint[4] if quint[4] else '__na__']\n",
    "        ] \n",
    "        _negative.append(_neg)\n",
    "    _negatives.append(_negative)\n",
    "\n",
    "# _positives, _negatives = np.array(_positives), np.array(_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[133361, 268, 24127, 0, 0],\n",
       "  [133361, 87, 43032, 0, 0],\n",
       "  [133361, 207, 136914, 0, 0],\n",
       "  [133361, 327, 113545, 0, 0],\n",
       "  [133361, 55, 137231, 0, 0],\n",
       "  [133361, 23, 38966, 0, 0],\n",
       "  [133361, 23, 110846, 0, 0],\n",
       "  [133361, 23, 134072, 0, 0],\n",
       "  [133361, 23, 42364, 0, 0],\n",
       "  [133361, 55, 160056, 0, 0]],\n",
       " [[133361, 268, 24127, 188, 33440],\n",
       "  [133361, 268, 2518, 0, 0],\n",
       "  [133361, 268, 24127, 51, 64654],\n",
       "  [27091, 268, 24127, 0, 0],\n",
       "  [133361, 268, 24127, 652, 30],\n",
       "  [133361, 268, 24127, 273, 84215],\n",
       "  [133361, 268, 24127, 296, 170213],\n",
       "  [133361, 268, 24127, 210, 101663],\n",
       "  [133361, 268, 24127, 585, 145390],\n",
       "  [133361, 268, 24127, 122, 79293]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_positives[:10], _negatives[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Convert raw_data (pos) and negatives to int lists/arrays\n",
    "_positives = [[uritoid[uri] if uri else uritoid['__na__'] for uri in quint] for quint in raw_data]\n",
    "_negatives = [[[uritoid[uri] if uri else uritoid['__na__'] for uri in quint] for quint in negative] for negative in negatives]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "_positives[:10], _negatives[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "771bbbf3b6384427b3e385c3af791420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=973), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should stop\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try sampling (test), skip in prod\n",
    "sampler = QuintRankingSampler({\"pos\": _positives, \"neg\": _negatives }, bs=4000)\n",
    "for x in tqdm(sampler):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Make a model.\n",
    "\n",
    "See if we can use something for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = EXPERIMENT_CONFIG.copy()\n",
    "config['DEVICE'] = torch.device('cpu')\n",
    "model = TransE(config)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=config['LEARNING_RATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_positives.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data in train and valid\n",
    "index = np.arange(len(_positives))\n",
    "np.random.shuffle(index)\n",
    "train_index, valid_index = index[:int(index.shape[0]*0.8)], index[int(index.shape[0]*0.8):]\n",
    "train_pos = [_positives[i] for i in train_index]\n",
    "valid_pos = [_positives[i] for i in valid_index]\n",
    "train_neg = [_negatives[i] for i in train_index]\n",
    "valid_neg = [_negatives[i] for i in valid_index]\n",
    "data = {'train': {'pos': train_pos, 'neg': train_neg}, 'valid': {'pos': valid_pos, 'neg': valid_neg}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(*args, **kwargs):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a loop fn\n",
    "def simplest_loop(epochs: int,\n",
    "                  data: dict,\n",
    "                  opt: torch.optim,\n",
    "                  train_fn: Callable,\n",
    "                  predict_fn: Callable,\n",
    "                  device: torch.device = torch.device('cpu'),\n",
    "                  data_fn: classmethod = dataiters.SimplestSampler,\n",
    "                  eval_fn: Callable = default_eval) -> (list, list, list):\n",
    "    \"\"\"\n",
    "        A fn which can be used to train a language model.\n",
    "\n",
    "        The model doesn't need to be an nn.Module,\n",
    "            but have an eval (optional), a train and a predict function.\n",
    "\n",
    "        Data should be a dict like so:\n",
    "            {\"train\":{\"x\":np.arr, \"y\":np.arr}, \"val\":{\"x\":np.arr, \"y\":np.arr} }\n",
    "\n",
    "        Train_fn must return both loss and y_pred\n",
    "\n",
    "        :param epochs: number of epochs to train for\n",
    "        :param data: a dict having keys train_x, test_x, train_y, test_y\n",
    "        :param device: torch device to create new tensor from data\n",
    "        :param opt: optimizer\n",
    "        :param loss_fn: loss function\n",
    "        :param train_fn: function to call with x and y\n",
    "        :param predict_fn: function to call with x (test)\n",
    "        :param data_fn: a class to which we can pass X and Y, and get an iterator.\n",
    "        :param eval_fn: (optional) function which when given pred and true, returns acc\n",
    "        :return: traces\n",
    "    \"\"\"\n",
    "\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    valid_acc = []\n",
    "    lrs = []\n",
    "\n",
    "    # Epoch level\n",
    "    for e in range(epochs):\n",
    "\n",
    "        per_epoch_loss = []\n",
    "#         per_epoch_tr_acc = []\n",
    "\n",
    "        # Train\n",
    "        with Timer() as timer:\n",
    "\n",
    "            # Make data\n",
    "            trn_dl, val_dl = data_fn(data['train']), data_fn(data['valid'])\n",
    "\n",
    "            for pos, neg in tqdm(trn_dl):\n",
    "                opt.zero_grad()\n",
    "\n",
    "                _pos = torch.tensor(pos, dtype=torch.long, device=device)\n",
    "                _neg = torch.tensor(neg, dtype=torch.long, device=device)\n",
    "\n",
    "                loss = train_fn(_pos, _neg)\n",
    "#                 loss = loss_fn(y_pred, _y)\n",
    "\n",
    "#                 per_epoch_tr_acc.append(eval_fn(y_pred=y_pred, y_true=_y).item())\n",
    "                per_epoch_loss.append(loss.item())\n",
    "\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "#         # Val\n",
    "#         with torch.no_grad():\n",
    "\n",
    "#             per_epoch_vl_acc = []\n",
    "#             for x, y in tqdm(val_dl):\n",
    "#                 _x = torch.tensor(x, dtype=torch.long, device=device)\n",
    "#                 _y = torch.tensor(y, dtype=torch.long, device=device)\n",
    "\n",
    "#                 y_pred = predict_fn(_x)\n",
    "\n",
    "#                 per_epoch_vl_acc.append(eval_fn(y_pred, _y).item())\n",
    "\n",
    "        # Bookkeep\n",
    "#         train_acc.append(np.mean(per_epoch_tr_acc))\n",
    "        train_loss.append(np.mean(per_epoch_loss))\n",
    "#         valid_acc.append(np.mean(per_epoch_vl_acc))\n",
    "\n",
    "#         print(\"Epoch: %(epo)03d | Loss: %(loss).5f | Tr_c: %(tracc)0.5f | Vl_c: %(vlacc)0.5f | Time: %(time).3f min\"\n",
    "#               % {'epo': e,\n",
    "#                  'loss': float(np.mean(per_epoch_loss)),\n",
    "#                  'tracc': float(np.mean(per_epoch_tr_acc)),\n",
    "#                  'vlacc': float(np.mean(per_epoch_vl_acc)),\n",
    "#                  'time': timer.interval / 60.0})\n",
    "        print(\"Epoch: %(epo)03d | Loss: %(loss).5f | Time: %(time).3f min\"\n",
    "              % {'epo': e,\n",
    "                 'loss': float(np.mean(per_epoch_loss)),\n",
    "                 'time': timer.interval / 60.0})\n",
    "\n",
    "    return train_acc, valid_acc, train_loss"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "epochs: int,\n",
    "                  data: dict,\n",
    "                  opt: torch.optim,\n",
    "                  train_fn: Callable,\n",
    "                  predict_fn: Callable,\n",
    "                  device: torch.device = torch.device('cpu'),\n",
    "                  data_fn: classmethod = dataiters.SimplestSampler,\n",
    "                  eval_fn: Callable = default_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"epochs\":2,\n",
    "    \"data\":data,\n",
    "    \"opt\": optimizer,\n",
    "    \"train_fn\": model,\n",
    "    \"predict_fn\": model.predict,\n",
    "    \"device\": config['DEVICE'],\n",
    "    \"data_fn\": partial(QuintRankingSampler, bs=config[\"BATCH_SIZE\"]),\n",
    "    \"eval_fn\": evaluate\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b46f4241687c4ccf8493a1e6531b29fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12184), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-2d39cad86446>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msimplest_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-55-a7d8d9b8b80e>\u001b[0m in \u001b[0;36msimplest_loop\u001b[0;34m(epochs, data, opt, train_fn, predict_fn, device, data_fn, eval_fn)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0m_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_neg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;31m#                 loss = loss_fn(y_pred, _y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/conda/envs/all/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-aa4f730a9f79>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch_positives, batch_negatives)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_positives\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_negatives\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# Normalize embeddings of entities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mnorms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentity_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml_p_norm_entities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         self.entity_embeddings.weight.data = self.entity_embeddings.weight.data.div(\n\u001b[1;32m     54\u001b[0m             norms.view(self.num_entities, 1).expand_as(self.entity_embeddings.weight))\n",
      "\u001b[0;32m~/Dev/conda/envs/all/lib/python3.6/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[1;32m    696\u001b[0m             \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "simplest_loop(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
