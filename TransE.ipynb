{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransE\n",
    "\n",
    "**Data**: s, p, o, qp, qe\n",
    "\n",
    "$s,p,o,qe,qp \\in R^{d}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "1. Model\n",
    "2. Sampling Code\n",
    "\n",
    "\n",
    "**Model** from [pyKeen](https://github.com/SmartDataAnalytics/PyKEEN/blob/master/src/pykeen/kge_models/trans_e.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "# from dataclasses import dataclass\n",
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.autograd\n",
    "from torch import nn\n",
    "\n",
    "import pandas as  pd\n",
    "import pickle\n",
    "\n",
    "from typing import Optional, Union, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local imports\n",
    "from raw_parser import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Stats\n",
    "n_entities, n_relations etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from disk\n",
    "with open('./parsed_raw_data.pkl', 'rb') as f:\n",
    "    raw_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174951 659\n"
     ]
    }
   ],
   "source": [
    "entities, predicates = [], []\n",
    "\n",
    "for quint in raw_data:\n",
    "    entities += [quint[0], quint[2]]\n",
    "    if quint[4]:\n",
    "        entities.append(quint[4])\n",
    "        \n",
    "    predicates.append(quint[1])\n",
    "    if quint[3]: \n",
    "        predicates.append(quint[3])\n",
    "    \n",
    "entities = list(set(entities))\n",
    "predicates = list(set(predicates))\n",
    "        \n",
    "print(len(entities), len(predicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANS_E_CONFIG = {\n",
    "    'NUM_ENTITIES': len(entities),\n",
    "    'NUM_RELATIONS': len(predicates),\n",
    "    'EMBEDDING_DIM': 200,\n",
    "    'NORM_FOR_NORMALIZATION_OF_ENTITIES': 2,\n",
    "    'NORM_FOR_NORMALIZATION_OF_RELATIONS': 2,\n",
    "    'SCORING_FUNCTION_NORM': 1,\n",
    "    'MARGIN_LOSS': 4,\n",
    "    'LEARNING_RATE': 0.0001\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_triples(triples: torch.Tensor) -> List[torch.Tensor]:\n",
    "    \"\"\" Slice in 3 or 5 as needed \"\"\"\n",
    "    return triples[:,0], triples[:,1], triples[:,2], triples[:,3], triples[:, 4]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[40, 71, 47, 60, 82],\n",
       "         [14, 66, 86, 91, 81],\n",
       "         [14, 77,  8, 89, 72],\n",
       "         [31, 23,  0, 79, 61],\n",
       "         [14, 61, 66, 62, 17],\n",
       "         [30, 86, 61, 83, 40],\n",
       "         [ 5, 12, 16, 58, 29],\n",
       "         [62, 51, 53, 90, 52],\n",
       "         [99,  1,  5, 85, 43],\n",
       "         [49, 75, 36, 73, 37]]),\n",
       " (tensor([40, 14, 14, 31, 14, 30,  5, 62, 99, 49]),\n",
       "  tensor([71, 66, 77, 23, 61, 86, 12, 51,  1, 75]),\n",
       "  tensor([47, 86,  8,  0, 66, 61, 16, 53,  5, 36]),\n",
       "  tensor([60, 91, 89, 79, 62, 83, 58, 90, 85, 73]),\n",
       "  tensor([82, 81, 72, 61, 17, 40, 29, 52, 43, 37])))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test slice_triples\n",
    "t = torch.randint(0, 100, (10, 5))\n",
    "t, slice_triples(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModule(nn.Module):\n",
    "    \"\"\"A base class for all of the models.\"\"\"\n",
    "\n",
    "    margin_ranking_loss_size_average: bool = None\n",
    "    entity_embedding_max_norm: Optional[int] = None\n",
    "    entity_embedding_norm_type: int = 2\n",
    "    hyper_params = [TRANS_E_CONFIG['EMBEDDING_DIM'], \n",
    "                    TRANS_E_CONFIG['MARGIN_LOSS'], \n",
    "                    TRANS_E_CONFIG['LEARNING_RATE']]\n",
    "\n",
    "    def __init__(self, config: Dict) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Device selection\n",
    "        self.device = config['DEVICE']\n",
    "\n",
    "        # Loss\n",
    "        self.margin_loss = config['MARGIN_LOSS']\n",
    "        self.criterion = nn.MarginRankingLoss(\n",
    "            margin=self.margin_loss,\n",
    "            reduction='mean' if self.margin_ranking_loss_size_average else 'sum'\n",
    "        )\n",
    "\n",
    "        # Entity dimensions\n",
    "        #: The number of entities in the knowledge graph\n",
    "        self.num_entities = config['NUM_ENTITIES']\n",
    "        #: The number of unique relation types in the knowledge graph\n",
    "        self.num_relations = config['NUM_RELATIONS']\n",
    "        #: The dimension of the embeddings to generate\n",
    "        self.embedding_dim = config['EMBEDDING_DIM']\n",
    "\n",
    "        self.entity_embeddings = nn.Embedding(\n",
    "            self.num_entities,\n",
    "            self.embedding_dim,\n",
    "            norm_type=self.entity_embedding_norm_type,\n",
    "            max_norm=self.entity_embedding_max_norm,\n",
    "        )\n",
    "\n",
    "    def __init_subclass__(cls, **kwargs):  # noqa: D105\n",
    "        if not getattr(cls, 'model_name', None):\n",
    "            raise TypeError('missing model_name class attribute')\n",
    "\n",
    "    def _get_entity_embeddings(self, entities):\n",
    "        return self.entity_embeddings(entities).view(-1, self.embedding_dim)\n",
    "\n",
    "    def _compute_loss(self, positive_scores: torch.Tensor, negative_scores: torch.Tensor) -> torch.Tensor:\n",
    "        y = np.repeat([-1], repeats=positive_scores.shape[0])\n",
    "        y = torch.tensor(y, dtype=torch.float, device=self.device)\n",
    "\n",
    "        loss = self.criterion(positive_scores, negative_scores, y)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransE(nn.Module):\n",
    "    \"\"\"An implementation of TransE [borders2013]_.\n",
    "     This model considers a relation as a translation from the head to the tail entity.\n",
    "    .. [borders2013] Bordes, A., *et al.* (2013). `Translating embeddings for modeling multi-relational data\n",
    "                     <http://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data.pdf>`_\n",
    "                     . NIPS.\n",
    "    .. seealso::\n",
    "       - Alternative implementation in OpenKE: https://github.com/thunlp/OpenKE/blob/OpenKE-PyTorch/models/TransE.py\n",
    "    \"\"\"\n",
    "\n",
    "    model_name = 'TransE MM'\n",
    "    margin_ranking_loss_size_average: bool = True\n",
    "    entity_embedding_max_norm: Optional[int] = None\n",
    "    entity_embedding_norm_type: int = 2\n",
    "    \n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Embeddings\n",
    "        self.l_p_norm_entities = config['NORM_FOR_NORMALIZATION_OF_ENTITIES']\n",
    "        self.scoring_fct_norm = config['SCORING_FUNCTION_NORM']\n",
    "        self.relation_embeddings = nn.Embedding(config['NUM_RELATIONS'], config['EMBEDDING_DIM'])\n",
    "        \n",
    "        self.config = config\n",
    "\n",
    "        self._initialize()\n",
    "\n",
    "    def _initialize(self):\n",
    "        embeddings_init_bound = 6 / np.sqrt(self.config['EMBEDDING_DIM'])\n",
    "        nn.init.uniform_(\n",
    "            self.entity_embeddings.weight.data,\n",
    "            a=-embeddings_init_bound,\n",
    "            b=+embeddings_init_bound,\n",
    "        )\n",
    "        nn.init.uniform_(\n",
    "            self.relation_embeddings.weight.data,\n",
    "            a=-embeddings_init_bound,\n",
    "            b=+embeddings_init_bound,\n",
    "        )\n",
    "\n",
    "        norms = torch.norm(self.relation_embeddings.weight, \n",
    "                           p=self.config['NORM_FOR_NORMALIZATION_OF_RELATIONS'], dim=1).data\n",
    "        self.relation_embeddings.weight.data = self.relation_embeddings.weight.data.div(\n",
    "            norms.view(self.num_relations, 1).expand_as(self.relation_embeddings.weight))\n",
    "\n",
    "    def predict(self, triples):\n",
    "        scores = self._score_triples(triples)\n",
    "        return scores.detach().cpu().numpy()\n",
    "\n",
    "    def forward(self, batch_positives, batch_negatives):\n",
    "        # Normalize embeddings of entities\n",
    "        norms = torch.norm(self.entity_embeddings.weight, p=self.l_p_norm_entities, dim=1).data\n",
    "        self.entity_embeddings.weight.data = self.entity_embeddings.weight.data.div(\n",
    "            norms.view(self.num_entities, 1).expand_as(self.entity_embeddings.weight))\n",
    "\n",
    "        positive_scores = self._score_triples(batch_positives)\n",
    "        negative_scores = self._score_triples(batch_negatives)\n",
    "        loss = self._compute_loss(positive_scores=positive_scores, negative_scores=negative_scores)\n",
    "        return loss\n",
    "\n",
    "    def _score_triples(self, triples):\n",
    "        # TODO: fix/change\n",
    "        head_embeddings, relation_embeddings, tail_embeddings = self._get_triple_embeddings(triples)\n",
    "        scores = self._compute_scores(head_embeddings, relation_embeddings, tail_embeddings)\n",
    "        return scores\n",
    "\n",
    "    def _compute_scores(self, head_embeddings, relation_embeddings, tail_embeddings):\n",
    "        \"\"\"Compute the scores based on the head, relation, and tail embeddings.\n",
    "        :param head_embeddings: embeddings of head entities of dimension batchsize x embedding_dim\n",
    "        :param relation_embeddings: emebddings of relation embeddings of dimension batchsize x embedding_dim\n",
    "        :param tail_embeddings: embeddings of tail entities of dimension batchsize x embedding_dim\n",
    "        :return: Tensor of dimension batch_size containing the scores for each batch element\n",
    "        \"\"\"\n",
    "        # Add the vector element wise\n",
    "        sum_res = head_embeddings + relation_embeddings - tail_embeddings\n",
    "        distances = torch.norm(sum_res, dim=1, p=self.scoring_fct_norm).view(size=(-1,))\n",
    "        return distances\n",
    "\n",
    "    def _get_triple_embeddings(self, triples):\n",
    "        heads, relations, tails = slice_triples(triples)\n",
    "        return (\n",
    "            self._get_entity_embeddings(heads),\n",
    "            self._get_relation_embeddings(relations),\n",
    "            self._get_entity_embeddings(tails),\n",
    "        )\n",
    "\n",
    "    def _get_relation_embeddings(self, relations):\n",
    "        return self.relation_embeddings(relations).view(-1, self.embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(10))[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
