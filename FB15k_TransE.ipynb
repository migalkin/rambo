{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone OpenKE for benchmark datasets FB15K-237 and WN18"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!git clone https://github.com/thunlp/OpenKE.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union, List, Callable, Dict\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "import pandas as  pd\n",
    "import numpy as np\n",
    "import traceback\n",
    "import warnings\n",
    "import logging\n",
    "import random\n",
    "import pickle\n",
    "import wandb\n",
    "\n",
    "\n",
    "\n",
    "# MyTorch imports\n",
    "from mytorch.utils.goodies import *\n",
    "from mytorch import dataiters\n",
    "\n",
    "# Local imports \n",
    "from raw_parser import Quint\n",
    "from utils import *\n",
    "from evaluation import *\n",
    "from models import TransE\n",
    "from corruption import Corruption\n",
    "from sampler import SimpleSampler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwriting data dir\n",
    "RAW_DATA_DIR = Path('./data/raw_data/fb15k237')\n",
    "DATASET = 'fb15k237'\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_triples = []\n",
    "valid_triples = []\n",
    "test_triples = []\n",
    "\n",
    "with open(RAW_DATA_DIR / \"entity2id.txt\", \"r\") as ent_file, \\\n",
    "    open(RAW_DATA_DIR / \"relation2id.txt\", \"r\") as rel_file, \\\n",
    "    open(RAW_DATA_DIR / \"train2id.txt\", \"r\") as train_file, \\\n",
    "    open(RAW_DATA_DIR / \"valid2id.txt\", \"r\") as valid_file, \\\n",
    "    open(RAW_DATA_DIR / \"test2id.txt\", \"r\") as test_file:\n",
    "    num_entities = int(next(ent_file).strip(\"\\n\"))\n",
    "    num_relations = int(next(rel_file).strip(\"\\n\"))\n",
    "    num_trains = int(next(train_file).strip(\"\\n\"))\n",
    "    for line in train_file:\n",
    "        triple = line.strip(\"\\n\").split(\" \")\n",
    "        training_triples.append([int(triple[0]), int(triple[2]), int(triple[1])])\n",
    "        \n",
    "    num_valid = int(next(valid_file).strip(\"\\n\"))\n",
    "    for line in valid_file:\n",
    "        triple = line.strip(\"\\n\").split(\" \")\n",
    "        valid_triples.append([int(triple[0]), int(triple[2]), int(triple[1])])\n",
    "    \n",
    "    num_test = int(next(test_file).strip(\"\\n\"))\n",
    "    for line in test_file:\n",
    "        triple = line.strip(\"\\n\").split(\" \")\n",
    "        test_triples.append([int(triple[0]), int(triple[2]), int(triple[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_CONFIG = {\n",
    "    'NUM_ENTITIES': num_entities,\n",
    "    'NUM_RELATIONS': num_relations,\n",
    "    'EMBEDDING_DIM': 200,\n",
    "    'NORM_FOR_NORMALIZATION_OF_ENTITIES': 2,\n",
    "    'NORM_FOR_NORMALIZATION_OF_RELATIONS': 2,\n",
    "    'SCORING_FUNCTION_NORM': 1,\n",
    "    'MARGIN_LOSS': 1,\n",
    "    'LEARNING_RATE': 0.001,\n",
    "    'NEGATIVE_SAMPLING_PROBS': [0.3, 0.0, 0.2, 0.5],\n",
    "    'NEGATIVE_SAMPLING_TIMES': 10,\n",
    "    'BATCH_SIZE': 5,\n",
    "    'EPOCHS': 100,\n",
    "    'IS_QUINTS': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_triples.__len__(), valid_triples.__len__()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def sample_negatives(triple: List) -> List:\n",
    "    if np.random.random() < 0.5:\n",
    "        # sample subject\n",
    "        return [random.choice(range(num_entities)), triple[1], triple[2]]\n",
    "    else:\n",
    "        # sample object\n",
    "        return [triple[0], triple[1], random.choice(range(num_entities))]   "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def generate_negatives(positive: List[List], times: int):\n",
    "    \"\"\"\n",
    "        :param postive: List of the raw data\n",
    "        :param times: how many negative samples per positive sample.\n",
    "    \"\"\"\n",
    "    negatives = []\n",
    "    for pos in tqdm(positive):\n",
    "        negatives_per_pos = [sample_negatives(pos) for _ in range(times)]\n",
    "        negatives.append(negatives_per_pos)\n",
    "        \n",
    "    return negatives"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "try:\n",
    "    negatives = pickle.load(open(PRETRAINING_DATA_DIR / 'fb15k_negatives.pkl', 'rb'))\n",
    "except (FileNotFoundError, IOError) as e:\n",
    "    # Generate it again\n",
    "    warnings.warn(\"Negative data not pre-generating. Takes three minutes.\")\n",
    "    negatives = generate_negatives(training_triples + valid_triples, \n",
    "                                   times = EXPERIMENT_CONFIG['NEGATIVE_SAMPLING_TIMES'])\n",
    "\n",
    "    # Dump this somewhere\n",
    "    with open(PRETRAINING_DATA_DIR / 'fb15k_negatives.pkl', 'wb+') as f:\n",
    "        pickle.dump(negatives, f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_neg = negatives[:len(training_triples)]\n",
    "val_neg = negatives[len(training_triples):]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data = {'train': {'pos': training_triples, 'neg': train_neg}, 'valid': {'pos': valid_triples, 'neg': val_neg}}\n",
    "data_pos = {'train': np.array(training_triples), 'valid': np.array(valid_triples)}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.append(np.array(data_pos['train']), data_pos['valid'], axis=0).shape, len(data_pos['train']), data_pos['valid'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = EXPERIMENT_CONFIG.copy()\n",
    "config['DEVICE'] = torch.device('cpu')\n",
    "model = TransE(config)\n",
    "model.to(config['DEVICE'])\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=config['LEARNING_RATE'])\n",
    "\n",
    "\n",
    "# wandb.init(project=\"wikidata-embeddings\")\n",
    "# for k, v in config.items():\n",
    "#     wandb.config[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'train': np.array(training_triples), 'valid': np.array(valid_triples)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics = [acc, mrr, partial(hits_at, k=3), partial(hits_at, k=5), partial(hits_at, k=10)]\n",
    "evaluation = EvaluationBench(data, model, config[\"BATCH_SIZE\"], metrics=eval_metrics, _filtered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a loop fn\n",
    "def simplest_loop(epochs: int,\n",
    "                  data: dict,\n",
    "                  opt: torch.optim,\n",
    "                  train_fn: Callable,\n",
    "                  predict_fn: Callable,\n",
    "                  neg_generator: Callable,\n",
    "                  device: torch.device = torch.device('cpu'),\n",
    "                  data_fn: Callable = dataiters.SimplestSampler,\n",
    "                  eval_fn_trn: Callable = default_eval,\n",
    "                  eval_fn_val: Callable = default_eval,\n",
    "                  eval_every: int = 1) -> (list, list, list):\n",
    "    \"\"\"\n",
    "        A fn which can be used to train a language model.\n",
    "\n",
    "        The model doesn't need to be an nn.Module,\n",
    "            but have an eval (optional), a train and a predict function.\n",
    "\n",
    "        Data should be a dict like so:\n",
    "            {\"train\":{\"x\":np.arr, \"y\":np.arr}, \"val\":{\"x\":np.arr, \"y\":np.arr} }\n",
    "\n",
    "        Train_fn must return both loss and y_pred\n",
    "            \n",
    "        :param @TODO\n",
    "    \"\"\"\n",
    "\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    valid_acc = []\n",
    "    valid_mrr = []\n",
    "    valid_hits_3, valid_hits_5, valid_hits_10 = [], [], []\n",
    "    lrs = []\n",
    "\n",
    "    # Epoch level\n",
    "    for e in range(epochs):\n",
    "\n",
    "        per_epoch_loss = []\n",
    "        per_epoch_tr_acc = []\n",
    "\n",
    "        # Train\n",
    "        with Timer() as timer:\n",
    "\n",
    "            # Make data\n",
    "            trn_dl = data_fn(data['train'])\n",
    "\n",
    "            for pos in tqdm(trn_dl):\n",
    "                neg = neg_generator.corrupt_batch(pos)\n",
    "                opt.zero_grad()\n",
    "\n",
    "                _pos = torch.tensor(pos, dtype=torch.long, device=device)\n",
    "                _neg = torch.tensor(neg, dtype=torch.long, device=device)\n",
    "\n",
    "                (pos_scores, neg_scores), loss = train_fn(_pos, _neg)\n",
    "\n",
    "                per_epoch_tr_acc.append(eval_fn_trn(pos_scores=pos_scores, neg_scores=neg_scores))\n",
    "                per_epoch_loss.append(loss.item())\n",
    "\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "        \"\"\"\n",
    "            # Val\n",
    "            Run through the dataset twice.\n",
    "                1. same as training data (pointwise eval)\n",
    "                2. One quint (pos+negs) at a time. \n",
    "        \"\"\" \n",
    "        if e % eval_every == 0:\n",
    "            with torch.no_grad():\n",
    "                summary = eval_fn_val()\n",
    "                per_epoch_vl_acc = summary['metrics']['acc']\n",
    "                per_epoch_vl_mrr = summary['metrics']['mrr']\n",
    "                per_epoch_vl_hits_3 = summary['metrics']['hits_at 3']\n",
    "                per_epoch_vl_hits_5 = summary['metrics']['hits_at 5']\n",
    "                per_epoch_vl_hits_10 = summary['metrics']['hits_at 10']\n",
    "\n",
    "#             per_epoch_vl_acc, per_epoch_vl_mrr, per_epoch_vl_acc_like_trn = [], [], []\n",
    "#             for quints in tqdm(val_dl):\n",
    "#                 _quints = torch.tensor(quints, dtype=torch.long, device=device)\n",
    "                \n",
    "#                 # Flatten it\n",
    "#                 _quints_shape = _quints.shape\n",
    "#                 scores = predict_fn(_quints.view(-1, _quints_shape[-1]))\n",
    "#                 scores = scores.view(_quints_shape[0], _quints_shape[1])\n",
    "                \n",
    "#                 accuracy, recirank = eval_fn_val(scores)\n",
    "\n",
    "#                 per_epoch_vl_acc.append(accuracy)\n",
    "#                 per_epoch_vl_mrr.append(recirank)\n",
    "            \n",
    "#             for pos, neg in tqdm(val_dl_like_trn):\n",
    "                \n",
    "#                 _pos = torch.tensor(pos, dtype=torch.long, device=device)\n",
    "#                 _neg = torch.tensor(neg, dtype=torch.long, device=device)\n",
    "\n",
    "#                 (pos_scores, neg_scores), loss = train_fn(_pos, _neg)\n",
    "#                 acc = eval_fn_trn(pos_scores=pos_scores, neg_scores=neg_scores)\n",
    "                \n",
    "#                 per_epoch_vl_acc_like_trn.append(acc)\n",
    "            \n",
    "\n",
    "        # Bookkeep\n",
    "        train_acc.append(np.mean(per_epoch_tr_acc))\n",
    "        train_loss.append(np.mean(per_epoch_loss))\n",
    "        \n",
    "        if e % eval_every == 0:\n",
    "            valid_acc.append(per_epoch_vl_acc)\n",
    "            valid_mrr.append(per_epoch_vl_mrr)\n",
    "            valid_hits_3.append(per_epoch_vl_hits_3)\n",
    "            valid_hits_5.append(per_epoch_vl_hits_5)\n",
    "            valid_hits_10.append(per_epoch_vl_hits_10)\n",
    "        \n",
    "            print(\"Epoch: %(epo)03d | Loss: %(loss).5f | Tr_c: %(tracc)0.5f | \"\n",
    "                  \"Vl_c: %(vlacc)0.5f | Vl_mrr: %(vlmrr)0.5f | \"\n",
    "                  \"Vl_h3: %(vlh3)0.5f | Vl_h5: %(vlh5)0.5f | Vl_h10: %(vlh10)0.5f | \"\n",
    "                  \"Time_Train: %(time).3f min\"\n",
    "                  % {'epo': e,\n",
    "                     'loss': float(np.mean(per_epoch_loss)),\n",
    "                     'tracc': float(np.mean(per_epoch_tr_acc)),\n",
    "                     'vlacc': float(per_epoch_vl_acc),\n",
    "                     'vlmrr': float(per_epoch_vl_mrr),\n",
    "                     'vlh3': float(per_epoch_vl_hits_3),\n",
    "                     'vlh5': float(per_epoch_vl_hits_5),\n",
    "                     'vlh10': float(per_epoch_vl_hits_10),\n",
    "                     'time': timer.interval / 60.0})\n",
    "\n",
    "            # Wandb stuff\n",
    "            wandb.log({\n",
    "                'epoch': e, \n",
    "                'loss': float(np.mean(per_epoch_loss)),\n",
    "                'trn_acc': float(np.mean(per_epoch_tr_acc)),\n",
    "                'val_acc': float(per_epoch_vl_acc),\n",
    "                'val_mrr': float(per_epoch_vl_mrr),\n",
    "                'val_hits@3': float(per_epoch_vl_hits_3),\n",
    "                'val_hits@5': float(per_epoch_vl_hits_5),\n",
    "                'val_hits@10': float(per_epoch_vl_hits_10),\n",
    "            })\n",
    "            \n",
    "        else:\n",
    "            print(\"Epoch: %(epo)03d | Loss: %(loss).5f | Tr_c: %(tracc)0.5f | \"\n",
    "                  \"Time_Train: %(time).3f min\"\n",
    "                  % {'epo': e,\n",
    "                     'loss': float(np.mean(per_epoch_loss)),\n",
    "                     'tracc': float(np.mean(per_epoch_tr_acc)),\n",
    "                     'time': timer.interval / 60.0})\n",
    "\n",
    "            # Wandb stuff\n",
    "            wandb.log({\n",
    "                'epoch': e, \n",
    "                'loss': float(np.mean(per_epoch_loss)),\n",
    "                'trn_acc': float(np.mean(per_epoch_tr_acc))\n",
    "            })\n",
    "        \n",
    "\n",
    "    return train_acc, valid_acc, valid_acc_like_trn, valid_mrr, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"epochs\":config['EPOCHS'],\n",
    "    \"data\":data,\n",
    "    \"opt\": optimizer,\n",
    "    \"train_fn\": model,\n",
    "    \"predict_fn\": model.predict,\n",
    "    \"device\": config['DEVICE'],\n",
    "    \"data_fn\": partial(SimpleSampler, bs=config[\"BATCH_SIZE\"]),\n",
    "    \"eval_fn_trn\": evaluate_pointwise,\n",
    "    \"eval_fn_val\": evaluation.run,\n",
    "    \"eval_every\": 20,\n",
    "    \"neg_generator\": Corruption(n=num_entities, position=[0, 2]) # unfiltered for train\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplest_loop(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
