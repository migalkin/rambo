{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone OpenKE for benchmark datasets FB15K-237 and WN18"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!git clone https://github.com/thunlp/OpenKE.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union, List, Callable, Dict\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "import pandas as  pd\n",
    "import numpy as np\n",
    "import traceback\n",
    "import warnings\n",
    "import logging\n",
    "import random\n",
    "import pickle\n",
    "import wandb\n",
    "\n",
    "\n",
    "\n",
    "# MyTorch imports\n",
    "from mytorch.utils.goodies import *\n",
    "from mytorch import dataiters\n",
    "\n",
    "# Local imports \n",
    "from raw_parser import Quint\n",
    "from utils import *\n",
    "from evaluation import *\n",
    "from models import TransE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwriting data dir\n",
    "RAW_DATA_DIR = Path('./data/FB15K237')\n",
    "DATASET = 'fb15k'\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_triples = []\n",
    "valid_triples = []\n",
    "test_triples = []\n",
    "\n",
    "with open(RAW_DATA_DIR / \"entity2id.txt\", \"r\") as ent_file, \\\n",
    "    open(RAW_DATA_DIR / \"relation2id.txt\", \"r\") as rel_file, \\\n",
    "    open(RAW_DATA_DIR / \"train2id.txt\", \"r\") as train_file, \\\n",
    "    open(RAW_DATA_DIR / \"valid2id.txt\", \"r\") as valid_file, \\\n",
    "    open(RAW_DATA_DIR / \"test2id.txt\", \"r\") as test_file:\n",
    "    num_entities = int(next(ent_file).strip(\"\\n\"))\n",
    "    num_relations = int(next(rel_file).strip(\"\\n\"))\n",
    "    num_trains = int(next(train_file).strip(\"\\n\"))\n",
    "    for line in train_file:\n",
    "        triple = line.strip(\"\\n\").split(\" \")\n",
    "        training_triples.append([int(triple[0]), int(triple[2]), int(triple[1])])\n",
    "        \n",
    "    num_valid = int(next(valid_file).strip(\"\\n\"))\n",
    "    for line in valid_file:\n",
    "        triple = line.strip(\"\\n\").split(\" \")\n",
    "        valid_triples.append([int(triple[0]), int(triple[2]), int(triple[1])])\n",
    "    \n",
    "    num_test = int(next(test_file).strip(\"\\n\"))\n",
    "    for line in test_file:\n",
    "        triple = line.strip(\"\\n\").split(\" \")\n",
    "        test_triples.append([int(triple[0]), int(triple[2]), int(triple[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_CONFIG = {\n",
    "    'NUM_ENTITIES': num_entities,\n",
    "    'NUM_RELATIONS': num_relations,\n",
    "    'EMBEDDING_DIM': 200,\n",
    "    'NORM_FOR_NORMALIZATION_OF_ENTITIES': 2,\n",
    "    'NORM_FOR_NORMALIZATION_OF_RELATIONS': 2,\n",
    "    'SCORING_FUNCTION_NORM': 1,\n",
    "    'MARGIN_LOSS': 4,\n",
    "    'LEARNING_RATE': 0.001,\n",
    "    'NEGATIVE_SAMPLING_PROBS': [0.3, 0.0, 0.2, 0.5],\n",
    "    'NEGATIVE_SAMPLING_TIMES': 10,\n",
    "    'BATCH_SIZE': 8192,\n",
    "    'EPOCHS': 100,\n",
    "    'IS_QUINTS': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_negatives(triple: List) -> List:\n",
    "    if np.random.random() < 0.5:\n",
    "        # sample subject\n",
    "        return [random.choice(range(num_entities)), triple[1], triple[2]]\n",
    "    else:\n",
    "        # sample object\n",
    "        return [triple[0], triple[1], random.choice(range(num_entities))]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_negatives(positive: List[List], times: int):\n",
    "    \"\"\"\n",
    "        :param postive: List of the raw data\n",
    "        :param times: how many negative samples per positive sample.\n",
    "    \"\"\"\n",
    "    negatives = []\n",
    "    for pos in tqdm(positive):\n",
    "        negatives_per_pos = [sample_negatives(pos) for _ in range(times)]\n",
    "        negatives.append(negatives_per_pos)\n",
    "        \n",
    "    return negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    negatives = pickle.load(open(PRETRAINING_DATA_DIR / 'fb15k_negatives.pkl', 'rb'))\n",
    "except (FileNotFoundError, IOError) as e:\n",
    "    # Generate it again\n",
    "    warnings.warn(\"Negative data not pre-generating. Takes three minutes.\")\n",
    "    negatives = generate_negatives(training_triples + valid_triples, \n",
    "                                   times = EXPERIMENT_CONFIG['NEGATIVE_SAMPLING_TIMES'])\n",
    "\n",
    "    # Dump this somewhere\n",
    "    with open(PRETRAINING_DATA_DIR / 'fb15k_negatives.pkl', 'wb+') as f:\n",
    "        pickle.dump(negatives, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_neg = negatives[:len(training_triples)]\n",
    "val_neg = negatives[len(training_triples):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'train': {'pos': training_triples, 'neg': train_neg}, 'valid': {'pos': valid_triples, 'neg': val_neg}}\n",
    "data_pos = {'train': training_triples, 'valid': np.array(valid_triples)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_triples[0:5], train_neg[0:5], valid_triples[0:5], val_neg[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = EXPERIMENT_CONFIG.copy()\n",
    "config['DEVICE'] = torch.device('cuda')\n",
    "model = TransE(config)\n",
    "model.to(config['DEVICE'])\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=config['LEARNING_RATE'])\n",
    "\n",
    "\n",
    "wandb.init(project=\"wikidata-embeddings\")\n",
    "for k, v in config.items():\n",
    "    wandb.config[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics = [acc, mrr, partial(hits_at, k=3), partial(hits_at, k=5), partial(hits_at, k=10)]\n",
    "evaluation = EvaluationBench(data_pos, model, config[\"BATCH_SIZE\"], metrics=eval_metrics, _quints=False, _filtered=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a loop fn\n",
    "def simplest_loop(epochs: int,\n",
    "                  data: dict,\n",
    "                  opt: torch.optim,\n",
    "                  train_fn: Callable,\n",
    "                  predict_fn: Callable,\n",
    "                  device: torch.device = torch.device('cpu'),\n",
    "                  data_fn: Callable = dataiters.SimplestSampler,\n",
    "                  data_fn_val: Callable = dataiters.SimplestSampler,\n",
    "                  eval_fn_trn: Callable = default_eval,\n",
    "                  eval_fn_val: Callable = default_eval,\n",
    "                  eval_every: int = 1) -> (list, list, list):\n",
    "    \"\"\"\n",
    "        A fn which can be used to train a language model.\n",
    "\n",
    "        The model doesn't need to be an nn.Module,\n",
    "            but have an eval (optional), a train and a predict function.\n",
    "\n",
    "        Data should be a dict like so:\n",
    "            {\"train\":{\"x\":np.arr, \"y\":np.arr}, \"val\":{\"x\":np.arr, \"y\":np.arr} }\n",
    "\n",
    "        Train_fn must return both loss and y_pred\n",
    "\n",
    "        :param epochs: number of epochs to train for\n",
    "        :param data: a dict having keys train_x, test_x, train_y, test_y\n",
    "        :param device: torch device to create new tensor from data\n",
    "        :param opt: optimizer\n",
    "        :param loss_fn: loss function\n",
    "        :param train_fn: function to call with x and y\n",
    "        :param predict_fn: function to call with x (test)\n",
    "        :param data_fn: a class to which we can pass training data and get an iterator.\n",
    "        :param data_fn_val: can be same as above; or diff. Specifically for validation runs.\n",
    "        :param eval_fn: (optional) function which when given pred and true, returns acc\n",
    "        :return: traces\n",
    "    \"\"\"\n",
    "\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    valid_acc = []\n",
    "    valid_mrr = []\n",
    "    valid_hits_3, valid_hits_5, valid_hits_10 = [], [], []\n",
    "    lrs = []\n",
    "\n",
    "    # Epoch level\n",
    "    for e in range(epochs):\n",
    "\n",
    "        per_epoch_loss = []\n",
    "        per_epoch_tr_acc = []\n",
    "\n",
    "        # Train\n",
    "        with Timer() as timer:\n",
    "\n",
    "            # Make data\n",
    "            trn_dl, val_dl = data_fn(data['train']), data_fn_val(data['valid'])\n",
    "\n",
    "            for pos, neg in tqdm(trn_dl):\n",
    "                opt.zero_grad()\n",
    "\n",
    "                _pos = torch.tensor(pos, dtype=torch.long, device=device)\n",
    "                _neg = torch.tensor(neg, dtype=torch.long, device=device)\n",
    "\n",
    "                (pos_scores, neg_scores), loss = train_fn(_pos, _neg)\n",
    "\n",
    "                per_epoch_tr_acc.append(eval_fn_trn(pos_scores=pos_scores, neg_scores=neg_scores))\n",
    "                per_epoch_loss.append(loss.item())\n",
    "\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "        \"\"\"\n",
    "            # Val\n",
    "            Run through the dataset twice.\n",
    "                1. same as training data (pointwise eval)\n",
    "                2. One quint (pos+negs) at a time. \n",
    "        \"\"\" \n",
    "        if e % eval_every == 0:\n",
    "            with torch.no_grad():\n",
    "                summary = eval_fn_val(np.array(val_dl))\n",
    "                per_epoch_vl_acc = summary['metrics']['acc']\n",
    "                per_epoch_vl_mrr = summary['metrics']['mrr']\n",
    "                per_epoch_vl_hits_3 = summary['metrics']['hits_at 3']\n",
    "                per_epoch_vl_hits_5 = summary['metrics']['hits_at 5']\n",
    "                per_epoch_vl_hits_10 = summary['metrics']['hits_at 10']\n",
    "\n",
    "#             per_epoch_vl_acc, per_epoch_vl_mrr, per_epoch_vl_acc_like_trn = [], [], []\n",
    "#             for quints in tqdm(val_dl):\n",
    "#                 _quints = torch.tensor(quints, dtype=torch.long, device=device)\n",
    "                \n",
    "#                 # Flatten it\n",
    "#                 _quints_shape = _quints.shape\n",
    "#                 scores = predict_fn(_quints.view(-1, _quints_shape[-1]))\n",
    "#                 scores = scores.view(_quints_shape[0], _quints_shape[1])\n",
    "                \n",
    "#                 accuracy, recirank = eval_fn_val(scores)\n",
    "\n",
    "#                 per_epoch_vl_acc.append(accuracy)\n",
    "#                 per_epoch_vl_mrr.append(recirank)\n",
    "            \n",
    "#             for pos, neg in tqdm(val_dl_like_trn):\n",
    "                \n",
    "#                 _pos = torch.tensor(pos, dtype=torch.long, device=device)\n",
    "#                 _neg = torch.tensor(neg, dtype=torch.long, device=device)\n",
    "\n",
    "#                 (pos_scores, neg_scores), loss = train_fn(_pos, _neg)\n",
    "#                 acc = eval_fn_trn(pos_scores=pos_scores, neg_scores=neg_scores)\n",
    "                \n",
    "#                 per_epoch_vl_acc_like_trn.append(acc)\n",
    "            \n",
    "\n",
    "        # Bookkeep\n",
    "        train_acc.append(np.mean(per_epoch_tr_acc))\n",
    "        train_loss.append(np.mean(per_epoch_loss))\n",
    "        if e % eval_every == 0:\n",
    "            valid_acc.append(per_epoch_vl_acc)\n",
    "            valid_mrr.append(per_epoch_vl_mrr)\n",
    "            valid_hits_3.append(per_epoch_vl_hits_3)\n",
    "            valid_hits_5.append(per_epoch_vl_hits_5)\n",
    "            valid_hits_10.append(per_epoch_vl_hits_10)\n",
    "        \n",
    "            print(\"Epoch: %(epo)03d | Loss: %(loss).5f | Tr_c: %(tracc)0.5f | \"\n",
    "                  \"Vl_c: %(vlacc)0.5f | Vl_mrr: %(vlmrr)0.5f | \"\n",
    "                  \"Vl_h3: %(vlh3)0.5f | Vl_h5: %(vlh5)0.5f | Vl_h10: %(vlh10)0.5f | \"\n",
    "                  \"Time_Train: %(time).3f min\"\n",
    "                  % {'epo': e,\n",
    "                     'loss': float(np.mean(per_epoch_loss)),\n",
    "                     'tracc': float(np.mean(per_epoch_tr_acc)),\n",
    "                     'vlacc': float(per_epoch_vl_acc),\n",
    "                     'vlmrr': float(per_epoch_vl_mrr),\n",
    "                     'vlh3': float(per_epoch_vl_hits_3),\n",
    "                     'vlh5': float(per_epoch_vl_hits_5),\n",
    "                     'vlh10': float(per_epoch_vl_hits_10),\n",
    "                     'time': timer.interval / 60.0})\n",
    "\n",
    "            # Wandb stuff\n",
    "            wandb.log({\n",
    "                'epoch': e, \n",
    "                'loss': float(np.mean(per_epoch_loss)),\n",
    "                'trn_acc': float(np.mean(per_epoch_tr_acc)),\n",
    "                'val_acc': float(per_epoch_vl_acc),\n",
    "                'val_mrr': float(per_epoch_vl_mrr),\n",
    "                'val_hits@3': float(per_epoch_vl_hits_3),\n",
    "                'val_hits@5': float(per_epoch_vl_hits_5),\n",
    "                'val_hits@10': float(per_epoch_vl_hits_10),\n",
    "            })\n",
    "            \n",
    "        else:\n",
    "            print(\"Epoch: %(epo)03d | Loss: %(loss).5f | Tr_c: %(tracc)0.5f | \"\n",
    "                  \"Time_Train: %(time).3f min\"\n",
    "                  % {'epo': e,\n",
    "                     'loss': float(np.mean(per_epoch_loss)),\n",
    "                     'tracc': float(np.mean(per_epoch_tr_acc)),\n",
    "                     'time': timer.interval / 60.0})\n",
    "\n",
    "            # Wandb stuff\n",
    "            wandb.log({\n",
    "                'epoch': e, \n",
    "                'loss': float(np.mean(per_epoch_loss)),\n",
    "                'trn_acc': float(np.mean(per_epoch_tr_acc))\n",
    "            })\n",
    "        \n",
    "\n",
    "    return train_acc, valid_acc, valid_acc_like_trn, valid_mrr, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"epochs\":config['EPOCHS'],\n",
    "    \"data\":data,\n",
    "    \"opt\": optimizer,\n",
    "    \"train_fn\": model,\n",
    "    \"predict_fn\": model.predict,\n",
    "    \"device\": config['DEVICE'],\n",
    "    \"data_fn\": partial(QuintRankingSampler, bs=config[\"BATCH_SIZE\"]),\n",
    "    \"data_fn_val\": partial(SingleSampler, bs=config[\"BATCH_SIZE\"]),\n",
    "    \"eval_fn_trn\": evaluate_pointwise,\n",
    "    \"eval_fn_val\": evaluation.run,\n",
    "    \"eval_every\": 20\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplest_loop(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
