{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T15:48:30.522441Z",
     "start_time": "2019-10-14T15:48:30.514755Z"
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import random\n",
    "import wandb\n",
    "import sys\n",
    "\n",
    "# MyTorch imports\n",
    "from mytorch.utils.goodies import *\n",
    "\n",
    "# Local imports\n",
    "from parse_wd15k import Quint\n",
    "from load import DataManager\n",
    "from utils import *\n",
    "from evaluation import EvaluationBench, acc, mrr, mr, hits_at, evaluate_pointwise\n",
    "from models import TransE, BaseModule\n",
    "from corruption import Corruption\n",
    "from sampler import SimpleSampler\n",
    "from loops import training_loop\n",
    "\n",
    "\"\"\"\n",
    "    CONFIG Things\n",
    "\"\"\"\n",
    "\n",
    "# Clamp the randomness\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "\"\"\"\n",
    "    Explanation:\n",
    "        *ENT_POS_FILTERED* \n",
    "            a flag which if False, implies that while making negatives, \n",
    "                we should exclude entities that appear ONLY in non-corrupting positions.\n",
    "            Do not turn it off if the experiment is about predicting qualifiers, of course.\n",
    "\n",
    "        *POSITIONS*\n",
    "            the positions on which we should inflect the negatives.\n",
    "\"\"\"\n",
    "DEFAULT_CONFIG = {\n",
    "    'EMBEDDING_DIM': 50,\n",
    "    'NORM_FOR_NORMALIZATION_OF_ENTITIES': 2,\n",
    "    'NORM_FOR_NORMALIZATION_OF_RELATIONS': 2,\n",
    "    'SCORING_FUNCTION_NORM': 1,\n",
    "    'MARGIN_LOSS': 1,\n",
    "    'LEARNING_RATE': 0.001,\n",
    "    'NEGATIVE_SAMPLING_PROBS': [0.3, 0.0, 0.2, 0.5],\n",
    "    'NEGATIVE_SAMPLING_TIMES': 10,\n",
    "    'BATCH_SIZE': 64,\n",
    "    'EPOCHS': 1000,\n",
    "    'STATEMENT_LEN': -1,\n",
    "    'EVAL_EVERY': 10,\n",
    "    'WANDB': False,\n",
    "    'RUN_TESTBENCH_ON_TRAIN': True,\n",
    "    'DATASET': 'wd15k',\n",
    "    'CORRUPTION_POSITIONS': [0, 2],\n",
    "    'DEVICE': 'cpu',\n",
    "    'ENT_POS_FILTERED': True,\n",
    "    'USE_TEST': False,\n",
    "    'MAX_QPAIRS': 43,\n",
    "    'NUM_FILTERS': 10,\n",
    "    'PROJECT_QUALIFIERS': False,\n",
    "    'SELF_ATTENTION'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T15:48:32.668052Z",
     "start_time": "2019-10-14T15:48:31.097822Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43051\n"
     ]
    }
   ],
   "source": [
    "# Custom Sanity Checks\n",
    "if DEFAULT_CONFIG['DATASET'] == 'wd15k':\n",
    "    assert DEFAULT_CONFIG['STATEMENT_LEN'] is not None, \\\n",
    "        \"You use WD15k dataset and don't specify whether to treat them as quints or not. Nicht cool'\"\n",
    "if max(DEFAULT_CONFIG['CORRUPTION_POSITIONS']) > 2:     # If we're corrupting something apart from S and O\n",
    "    assert DEFAULT_CONFIG['ENT_POS_FILTERED'] is False, \\\n",
    "        f\"Since we're corrupting objects at pos. {DEFAULT_CONFIG['CORRUPTION_POSITIONS']}, \" \\\n",
    "        f\"You must allow including entities which appear exclusively in qualifiers, too!\"\n",
    "\n",
    "\"\"\"\n",
    "    Load data based on the args/config\n",
    "\"\"\"\n",
    "data = DataManager.load(config=DEFAULT_CONFIG)()\n",
    "try:\n",
    "    training_triples, valid_triples, test_triples, num_entities, num_relations, e2id, r2id = data.values()\n",
    "except ValueError:\n",
    "    raise ValueError(f\"Honey I broke the loader for {DEFAULT_CONFIG['DATASET']}\")\n",
    "\n",
    "if DEFAULT_CONFIG['ENT_POS_FILTERED']:\n",
    "    ent_excluded_from_corr = DataManager.gather_missing_entities(data=training_triples + valid_triples + test_triples,\n",
    "                                                                 positions=DEFAULT_CONFIG['CORRUPTION_POSITIONS'],\n",
    "                                                                 n_ents=num_entities)\n",
    "    DEFAULT_CONFIG['NUM_ENTITIES_FILTERED'] = len(ent_excluded_from_corr)\n",
    "else:\n",
    "    ent_excluded_from_corr = []\n",
    "    DEFAULT_CONFIG['NUM_ENTITIES_FILTERED'] = len(ent_excluded_from_corr)\n",
    "\n",
    "print(num_entities-DEFAULT_CONFIG['NUM_ENTITIES_FILTERED'])\n",
    "DEFAULT_CONFIG['NUM_ENTITIES'] = num_entities\n",
    "DEFAULT_CONFIG['NUM_RELATIONS'] = num_relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T15:48:32.672869Z",
     "start_time": "2019-10-14T15:48:32.670058Z"
    }
   },
   "outputs": [],
   "source": [
    "config = DEFAULT_CONFIG.copy()\n",
    "config['DEVICE'] = torch.device(config['DEVICE'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOUR MODEL COMES HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T15:48:32.677291Z",
     "start_time": "2019-10-14T15:48:32.674952Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.autograd\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T15:48:32.835508Z",
     "start_time": "2019-10-14T15:48:32.811761Z"
    }
   },
   "outputs": [],
   "source": [
    "class ConvKB(BaseModule):\n",
    "    \"\"\"\n",
    "    An implementation of ConvKB.\n",
    "    \n",
    "    A Novel Embedding Model for Knowledge Base CompletionBased on Convolutional Neural Network. \n",
    "    \"\"\"\n",
    "\n",
    "    model_name = 'ConvKB'\n",
    "\n",
    "    def __init__(self, config) -> None:\n",
    "\n",
    "        self.margin_ranking_loss_size_average: bool = True\n",
    "        self.entity_embedding_max_norm: Optional[int] = None\n",
    "        self.entity_embedding_norm_type: int = 2\n",
    "        self.model_name = 'ConvKB'\n",
    "        super().__init__(config)\n",
    "        self.statement_len = config['STATEMENT_LEN']\n",
    "\n",
    "        # Embeddings\n",
    "        self.l_p_norm_entities = config['NORM_FOR_NORMALIZATION_OF_ENTITIES']\n",
    "        self.scoring_fct_norm = config['SCORING_FUNCTION_NORM']\n",
    "        self.relation_embeddings = nn.Embedding(config['NUM_RELATIONS'], config['EMBEDDING_DIM'], padding_idx=0)\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        self.criterion = nn.SoftMarginLoss(\n",
    "            reduction='sum'\n",
    "        )\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels=1, \n",
    "                              out_channels=config['NUM_FILTER'], kernel_size= (config['MAX_QPAIRS'],1), \n",
    "                             bias=True)\n",
    "        \n",
    "        self.fc = nn.Linear(config['NUM_FILTER']*self.embedding_dim,1, bias=False)\n",
    "        \n",
    "        self._initialize()\n",
    "\n",
    "        # Make pad index zero. # TODO: Should pad index be configurable? Probably not, right? Cool? Cool.\n",
    "        # self.entity_embeddings.weight.data[0] = torch.zeros_like(self.entity_embeddings.weight[0], requires_grad=True)\n",
    "        # self.relation_embeddings.weight.data[0] = torch.zeros_like(self.relation_embeddings.weight[0], requires_grad=True)\n",
    "\n",
    "    def _initialize(self):\n",
    "        embeddings_init_bound = 6 / np.sqrt(self.config['EMBEDDING_DIM'])\n",
    "        nn.init.uniform_(\n",
    "            self.entity_embeddings.weight.data,\n",
    "            a=-embeddings_init_bound,\n",
    "            b=+embeddings_init_bound,\n",
    "        )\n",
    "        nn.init.uniform_(\n",
    "            self.relation_embeddings.weight.data,\n",
    "            a=-embeddings_init_bound,\n",
    "            b=+embeddings_init_bound,\n",
    "        )\n",
    "\n",
    "        norms = torch.norm(self.relation_embeddings.weight,\n",
    "                           p=self.config['NORM_FOR_NORMALIZATION_OF_RELATIONS'], dim=1).data\n",
    "        self.relation_embeddings.weight.data = self.relation_embeddings.weight.data.div(\n",
    "            norms.view(self.num_relations, 1).expand_as(self.relation_embeddings.weight))\n",
    "\n",
    "        self.relation_embeddings.weight.data[0] = torch.zeros(1, self.embedding_dim)\n",
    "        self.entity_embeddings.weight.data[0] = torch.zeros(1, self.embedding_dim)  # zeroing the padding index\n",
    "        \n",
    "        \n",
    "\n",
    "    def predict(self, triples):\n",
    "        scores = self._score_triples(triples)\n",
    "        return scores\n",
    "    \n",
    "    def _compute_loss(self, positive_scores: torch.Tensor, negative_scores: torch.Tensor) -> torch.Tensor:\n",
    "        # Let n items in pos score.\n",
    "        y = np.repeat([-1], repeats=positive_scores.shape[0])          # n item here (all -1)\n",
    "        y = torch.tensor(y, dtype=torch.float, device=self.device)\n",
    "\n",
    "        pos_loss = self.criterion(positive_scores, -1*y)\n",
    "        neg_loss = self.criterion(negative_scores, y)\n",
    "        return pos_loss + neg_loss\n",
    "\n",
    "    def forward(self, batch_positives, batch_negatives) \\\n",
    "            -> Tuple[Tuple[torch.Tensor, torch.Tensor], torch.Tensor]:\n",
    "\n",
    "        # Normalize embeddings of entities\n",
    "        norms = torch.norm(self.entity_embeddings.weight, p=self.l_p_norm_entities, dim=1).data\n",
    "        \n",
    "        self.entity_embeddings.weight.data = self.entity_embeddings.weight.data.div(\n",
    "            norms.view(self.num_entities, 1).expand_as(self.entity_embeddings.weight))\n",
    "        \n",
    "        self.entity_embeddings.weight.data[0] = torch.zeros(1, self.embedding_dim)  # zeroing the padding index\n",
    "\n",
    "        positive_scores = self._score_triples(batch_positives)\n",
    "        negative_scores = self._score_triples(batch_negatives)\n",
    "        loss = self._compute_loss(positive_scores=positive_scores, negative_scores=negative_scores)\n",
    "        return (positive_scores, negative_scores), loss\n",
    "\n",
    "    def _score_triples(self, triples) -> torch.Tensor:\n",
    "        \"\"\" Get triple/quint embeddings, and compute scores \"\"\"\n",
    "        scores = self._compute_scores(*self._get_triple_embeddings(triples))\n",
    "        return scores\n",
    "\n",
    "\n",
    "    def _compute_scores(self, head_embeddings, relation_embeddings, tail_embeddings,\n",
    "                        qual_relation_embeddings=None, qual_entity_embeddings=None):\n",
    "        \"\"\"\n",
    "            Compute the scores based on the head, relation, and tail embeddings.\n",
    "\n",
    "        :param head_embeddings: embeddings of head entities of dimension batchsize x embedding_dim\n",
    "        :param relation_embeddings: embeddings of relation embeddings of dimension batchsize x embedding_dim\n",
    "        :param tail_embeddings: embeddings of tail entities of dimension batchsize x embedding_dim\n",
    "        :param qual_entity_embeddings: embeddings of qualifier relations of dimensinos batchsize x embeddig_dim\n",
    "        :param qual_relation_embeddings: embeddings of qualifier entities of dimension batchsize x embedding_dim\n",
    "        :return: Tensor of dimension batch_size containing the scores for each batch element\n",
    "        \"\"\"\n",
    "        \n",
    "        statement_emb = torch.zeros(head_embeddings.shape[0],\n",
    "                                    relation_embeddings.shape[1]*2+1,\n",
    "                                     head_embeddings.shape[1], \n",
    "                                 device=self.config['DEVICE'],\n",
    "                                   dtype=head_embeddings.dtype) # 1 for head embedding\n",
    "        \n",
    "        # Assignment\n",
    "        statement_emb[:,0] = head_embeddings\n",
    "        statement_emb[:,1::2] = relation_embeddings\n",
    "        statement_emb[:,2::2] = tail_embeddings\n",
    "        \n",
    "        \n",
    "        # Convolutional operation\n",
    "        statement_emb = F.relu(self.conv(statement_emb.unsqueeze(1))).squeeze(-1) # bs*number_of_filter*emb_dim            \n",
    "        statement_emb = statement_emb.view(statement_emb.shape[0], -1)\n",
    "        score = self.fc(statement_emb)\n",
    "        \n",
    "        return score.squeeze()\n",
    "\n",
    "    def _get_triple_embeddings(self, triples):\n",
    "        \n",
    "        head, statement_entities, statement_relations = slice_triples(triples, -1)\n",
    "        return (\n",
    "            self._get_entity_embeddings(head),\n",
    "            self.relation_embeddings(statement_relations),\n",
    "            self.entity_embeddings(statement_entities)\n",
    "        )\n",
    "\n",
    "    def _get_relation_embeddings(self, relations):\n",
    "        return self.relation_embeddings(relations).view(-1, self.embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T15:48:33.869926Z",
     "start_time": "2019-10-14T15:48:33.822142Z"
    }
   },
   "outputs": [],
   "source": [
    "model = TransE(config)\n",
    "model.to(config['DEVICE'])\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=config['LEARNING_RATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T15:48:41.964287Z",
     "start_time": "2019-10-14T15:48:34.758324Z"
    }
   },
   "outputs": [],
   "source": [
    "data = {'index': np.array(training_triples + test_triples), 'eval': np.array(valid_triples)}\n",
    "_data = {'index': np.array(valid_triples + test_triples), 'eval': np.array(training_triples)}\n",
    "tr_data = {'train': np.array(training_triples), 'valid': data['eval']}\n",
    "\n",
    "eval_metrics = [acc, mrr, mr, partial(hits_at, k=3), partial(hits_at, k=5), partial(hits_at, k=10)]\n",
    "evaluation_valid = EvaluationBench(data, model, bs=8000,\n",
    "                                   metrics=eval_metrics, filtered=True,\n",
    "                                   n_ents=num_entities,\n",
    "                                   excluding_entities=ent_excluded_from_corr,\n",
    "                                   positions=config.get('CORRUPTION_POSITIONS', None))\n",
    "evaluation_train = EvaluationBench(_data, model, bs=8000,\n",
    "                                   metrics=eval_metrics, filtered=True,\n",
    "                                   n_ents=num_entities,\n",
    "                                   excluding_entities=ent_excluded_from_corr,\n",
    "                                   positions=config.get('CORRUPTION_POSITIONS', None), trim=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T15:48:54.405738Z",
     "start_time": "2019-10-14T15:48:54.400383Z"
    }
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "        \"epochs\": config['EPOCHS'],\n",
    "        \"data\": tr_data,\n",
    "        \"opt\": optimizer,\n",
    "        \"train_fn\": model,\n",
    "        \"neg_generator\": Corruption(n=num_entities, excluding=ent_excluded_from_corr,\n",
    "                                    position=list(range(0, config['MAX_QPAIRS'], 2))),\n",
    "        \"device\": config['DEVICE'],\n",
    "        \"data_fn\": partial(SimpleSampler, bs=config[\"BATCH_SIZE\"]),\n",
    "        \"eval_fn_trn\": evaluate_pointwise,\n",
    "        \"val_testbench\": evaluation_valid.run,\n",
    "        \"trn_testbench\": evaluation_train.run,\n",
    "        \"eval_every\": config['EVAL_EVERY'],\n",
    "        \"log_wandb\": config['WANDB'],\n",
    "        \"run_trn_testbench\": config['RUN_TESTBENCH_ON_TRAIN']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T15:48:55.337061Z",
     "start_time": "2019-10-14T15:48:54.982228Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a8783e6312c46e8a3c503be2699a06b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2646), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'SELF_ATTENTION'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-8314fb3682bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Dev/research/wikidata-embeddings/rambo/loops.py\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(epochs, data, opt, train_fn, neg_generator, device, data_fn, eval_fn_trn, val_testbench, trn_testbench, eval_every, log_wandb, run_trn_testbench, savedir, save_content)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0m_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mpos_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_neg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mper_epoch_tr_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_fn_trn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_scores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_scores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneg_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/conda/envs/all/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/research/wikidata-embeddings/rambo/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch_positives, batch_negatives)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentity_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# zeroing the padding index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0mpositive_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_score_triples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_positives\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0mnegative_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_score_triples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_negatives\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive_scores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpositive_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_scores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/research/wikidata-embeddings/rambo/models.py\u001b[0m in \u001b[0;36m_score_triples\u001b[0;34m(self, triples)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_score_triples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtriples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;34m\"\"\" Get triple/quint embeddings, and compute scores \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_triple_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtriples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/research/wikidata-embeddings/rambo/models.py\u001b[0m in \u001b[0;36m_compute_scores\u001b[0;34m(self, head_embeddings, relation_embeddings, tail_embeddings, qual_relation_embeddings, qual_entity_embeddings)\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0mp_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj_mat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_rel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0msum_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhead_embeddings\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mp_proj\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtail_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SELF_ATTENTION'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m                 \u001b[0msum_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_attention_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelation_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtail_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SELF_ATTENTION'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'SELF_ATTENTION'"
     ]
    }
   ],
   "source": [
    "traces = training_loop(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
