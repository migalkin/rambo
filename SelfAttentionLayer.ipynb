{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple self attention layer with masking and support for multi headed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T15:15:29.234828Z",
     "start_time": "2019-10-02T15:15:29.230824Z"
    }
   },
   "outputs": [],
   "source": [
    "# Torch imports\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.autograd\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from typing import List, Optional, Dict, Tuple\n",
    "\n",
    "# Local imports\n",
    "from utils import *\n",
    "\n",
    "\n",
    "from models import slice_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T09:36:10.863278Z",
     "start_time": "2019-10-02T09:36:10.860034Z"
    }
   },
   "outputs": [],
   "source": [
    "bs = 4\n",
    "n = 5\n",
    "ent_emb_dims = 3\n",
    "rel_emb_dims = 4\n",
    "out_features = 7\n",
    "alpha_leaky = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T09:36:10.872008Z",
     "start_time": "2019-10-02T09:36:10.865535Z"
    }
   },
   "outputs": [],
   "source": [
    "def self_attention_template():\n",
    "    # Setting things up\n",
    "    bs = 4\n",
    "    n = 5\n",
    "    ent_emb_dims = 3\n",
    "    rel_emb_dims = 4\n",
    "    out_features = 7\n",
    "    alpha_leaky = 0.2\n",
    "\n",
    "    matrix = torch.randn(bs,n,2*ent_emb_dims + rel_emb_dims) # concat s,p,o.\n",
    "    print(f\"shape of matrix is bs*n*emb_dim i.e {matrix.shape}\")\n",
    "    \n",
    "    # passing it through layer1\n",
    "    w1 = nn.Linear(2 * ent_emb_dim + rel_emb_dim, out_features)\n",
    "    nn.init.xavier_normal_(w1.weight.data, gain=1.414)\n",
    "\n",
    "    c = w1(matrix)\n",
    "    print(f\"shape of c is {c.shape}\")\n",
    "    \n",
    "    # passing it through layer2\n",
    "    w2 = nn.Linear(out_features,1)\n",
    "    nn.init.xavier_normal_(w2.weight.data, gain=1.414)\n",
    "\n",
    "    b = w2(c)\n",
    "    leaky_relu = nn.LeakyReLU(alpha_leaky)\n",
    "    b = leaky_relu(b).squeeze()\n",
    "    print(f\"shape of b is {b.shape}\")\n",
    "    \n",
    "    # There will be no masking here. So simply a softmax and then multiply and sum across n.\n",
    "    alphas = torch.softmax(b,dim=1)\n",
    "    h = torch.sum((alphas.unsqueeze(-1)*c),dim=1)\n",
    "    \n",
    "    print(f\"shape of final vector by {h.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T09:36:10.953642Z",
     "start_time": "2019-10-02T09:36:10.874775Z"
    }
   },
   "outputs": [],
   "source": [
    "self_attention_template()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T09:36:34.923973Z",
     "start_time": "2019-10-02T09:36:34.916228Z"
    }
   },
   "outputs": [],
   "source": [
    "def self_attention_template_multi_head(num_head, final_layer=False):\n",
    "    # Setting things up\n",
    "    bs = 4\n",
    "    n = 5\n",
    "    ent_emb_dims = 3\n",
    "    rel_emb_dims = 4\n",
    "    out_features = 7\n",
    "    alpha_leaky = 0.2\n",
    "\n",
    "    matrix = torch.randn(bs,n,2*ent_emb_dims + rel_emb_dims) # concat s,p,o.\n",
    "    print(f\"shape of matrix is bs*n*emb_dim i.e {matrix.shape}\")\n",
    "    \n",
    "    # passing it through layer1\n",
    "    w1 = nn.Linear(2 * ent_emb_dims + rel_emb_dims, out_features)\n",
    "    nn.init.xavier_normal_(w1.weight.data, gain=1.414)\n",
    "\n",
    "    c = w1(matrix)\n",
    "    print(f\"shape of c is {c.shape}\")\n",
    "    \n",
    "    # passing it through layer2\n",
    "    w2 = nn.Linear(out_features,num_head)\n",
    "    nn.init.xavier_normal_(w2.weight.data, gain=1.414)\n",
    "\n",
    "    b = w2(c)\n",
    "    leaky_relu = nn.LeakyReLU(alpha_leaky)\n",
    "    b = leaky_relu(b).squeeze()\n",
    "    \n",
    "    print(f\"shape of b is {b.shape}\")\n",
    "    \n",
    "    # There will be no masking here. So simply a softmax and then multiply and sum across n.\n",
    "    alphas = torch.softmax(b,dim=1)\n",
    "    print(f\"shape of alphas is {alphas.shape}\")\n",
    "    \n",
    "    h = torch.bmm(c.transpose(1,2),alphas)\n",
    "    print(f\"shape of h is {h.shape}\")\n",
    "    if not final_layer:\n",
    "        h = h.view(bs,-1)\n",
    "        h = F.elu(h)\n",
    "    else:\n",
    "        h = torch.mean(h, dim=-1)\n",
    "        \n",
    "    print(f\"shape of final vector by {h.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T09:38:26.711467Z",
     "start_time": "2019-10-02T09:38:26.702955Z"
    }
   },
   "outputs": [],
   "source": [
    "self_attention_template_multi_head(num_head=8, final_layer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T11:43:01.588120Z",
     "start_time": "2019-10-02T11:43:01.578358Z"
    },
    "code_folding": [
     23
    ]
   },
   "outputs": [],
   "source": [
    "class GraphAttentionLayerMultihead(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: dict, final_layer: bool = False):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Parse params\n",
    "        ent_emb_dim, rel_emb_dim = config['EMBEDDING_DIM'], config['EMBEDDING_DIM']\n",
    "        out_features = config['GATARGS']['OUT']\n",
    "        num_head = config['GATARGS']['HEAD']\n",
    "        alpha_leaky = config['GATARGS']['ALPHA']\n",
    "        \n",
    "        self.w1 = nn.Linear(2 * ent_emb_dim + rel_emb_dim, out_features)\n",
    "        self.w2 = nn.Linear(out_features, num_head)\n",
    "        self.relu = nn.LeakyReLU(alpha_leaky)\n",
    "\n",
    "        self.final = final_layer\n",
    "        \n",
    "        # Why copy un-necessary stuff\n",
    "        self.heads = num_head\n",
    "        \n",
    "        # Not initializing here. Should be called by main module\n",
    "    \n",
    "    def initialize(self):\n",
    "        nn.init.xavier_normal_(self.w1.weight.data, gain=1.414)\n",
    "        nn.init.xavier_normal_(self.w2.weight.data, gain=1.414)\n",
    "        \n",
    "    def forward(self, data: torch.Tensor, mask: torch.Tensor=None):\n",
    "        \"\"\" \n",
    "            data: size (batchsize, num_neighbors, 2*ent_emb+rel_emb) or (bs, n, emb)\n",
    "            mask: size (batchsize, num_neighbors)\n",
    "            \n",
    "            PS: num_neighbors is padded either with max neighbors or with a limit \n",
    "        \"\"\"\n",
    "        \n",
    "                                                      #data: bs, n, emb\n",
    "        c = self.w1(data)                                #c: bs, n, out_features\n",
    "        b = self.relu(self.w2(c)).squeeze()              #b: bs, n, num_heads\n",
    "        m = mask.unsqueeze(-1).repeat(1, 1, self.heads)  #m: bs, n, num_heads\n",
    "        alphas = masked_softmax(b, m, dim=1)             #Î±: bs, n, num_heads\n",
    "        \n",
    "        print(alphas)\n",
    "        print(mask)\n",
    "        \n",
    "        # BMM simultaneously weighs the triples and sums across neighbors\n",
    "        h = torch.bmm(c.transpose(1,2),alphas)          #h: bs, out_features, num_heads\n",
    "        \n",
    "        if self.final:\n",
    "            h = torch.mean(h, dim=-1)                   #h: bs, out_features\n",
    "        else:\n",
    "            h = F.elu(h.view(bs, -1))                   #h: bs, out_features*num_heads\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T11:43:01.936651Z",
     "start_time": "2019-10-02T11:43:01.930837Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    bs = 4\n",
    "    n = 5\n",
    "    ent_emb_dims = 3\n",
    "    rel_emb_dims = 4\n",
    "    out_features = 7\n",
    "    alpha_leaky = 0.2\n",
    "\n",
    "    attn = GraphAttentionLayerMultihead(ent_emb_dims, rel_emb_dims, \n",
    "                                        out_features, alpha_leaky, num_head=8, final_layer=False)\n",
    "    print(attn)\n",
    "\n",
    "    data = torch.randn(bs, n, 2*ent_emb_dims+rel_emb_dims)\n",
    "    data[0][2:] = 0\n",
    "    data[1][4:] = 0\n",
    "    data[-1][1:] = 0\n",
    "\n",
    "    mask = compute_mask(data)\n",
    "    mask_condensed = torch.mean(mask, dim=-1)\n",
    "\n",
    "    print(data.shape)\n",
    "    op = attn(data, mask_condensed)\n",
    "\n",
    "    op, op.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Model Shit\n",
    "\n",
    "# init\n",
    "\n",
    "\"\"\"\n",
    "    -> init transE thing.\n",
    "    -> load pretrained weights\n",
    "    -> init the GAT layers (2)\n",
    "    \n",
    "    -> initialize nicely everything\n",
    "\"\"\"\n",
    "\n",
    "# forward\n",
    "\"\"\"\n",
    "    data: (bs, 3) :triple; (bs, neighborhood, 3) for Os; (bs, neighborhood, 3) for Ss'.... (also for hop2)\n",
    "    gat1(neighborhoodO)\n",
    "    gat2(neighborhoodO)\n",
    "    \n",
    "    # Same for S?\n",
    "    \n",
    "    embed bs, 1 -> relations\n",
    "    \n",
    "    transE loss.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T11:50:27.538962Z",
     "start_time": "2019-10-02T11:50:27.534836Z"
    },
    "code_folding": [
     29
    ]
   },
   "outputs": [],
   "source": [
    "class KBGat(BaseModule):\n",
    "    \n",
    "    model_name = 'KBGAT'\n",
    "    \n",
    "    def __init__(self, config: dict, pretrained_embeddings=None) -> None:\n",
    "        \n",
    "        self.margin_ranking_loss_size_average: bool = True\n",
    "        self.entity_embedding_max_norm: Optional[int] = None\n",
    "        self.entity_embedding_norm_type: int = 2\n",
    "        self.model_name = 'KBGAT'\n",
    "        super().__init__(config)\n",
    "        self.statement_len = config['STATEMENT_LEN']\n",
    "\n",
    "        # Embeddings\n",
    "        self.l_p_norm_entities = config['NORM_FOR_NORMALIZATION_OF_ENTITIES']\n",
    "        self.scoring_fct_norm = config['SCORING_FUNCTION_NORM']\n",
    "        self.relation_embeddings = nn.Embedding(config['NUM_RELATIONS'], config['EMBEDDING_DIM'], padding_idx=0)\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        if self.config['PROJECT_QUALIFIERS']:\n",
    "            self.proj_mat = nn.Linear(2*self.embedding_dim, self.embedding_dim, bias=False)\n",
    "            \n",
    "        self.gat1 = GraphAttentionLayerMultihead(self.config, final_layer=False)\n",
    "        self.gat2 = GraphAttentionLayerMultihead(self.config, final_layer=True)\n",
    "        \n",
    "        # Put in weights\n",
    "        self._initialize(pretrained_embeddings)\n",
    "        \n",
    "    def _initialize(self, pretrained_embeddings):\n",
    "        if pretrained_embeddings is None:\n",
    "            embeddings_init_bound = 6 / np.sqrt(self.config['EMBEDDING_DIM'])\n",
    "            nn.init.uniform_(\n",
    "                self.entity_embeddings.weight.data,\n",
    "                a=-embeddings_init_bound,\n",
    "                b=+embeddings_init_bound,\n",
    "            )\n",
    "            nn.init.uniform_(\n",
    "                self.relation_embeddings.weight.data,\n",
    "                a=-embeddings_init_bound,\n",
    "                b=+embeddings_init_bound,\n",
    "            )\n",
    "\n",
    "            norms = torch.norm(self.relation_embeddings.weight,\n",
    "                               p=self.config['NORM_FOR_NORMALIZATION_OF_RELATIONS'], dim=1).data\n",
    "            self.relation_embeddings.weight.data = self.relation_embeddings.weight.data.div(\n",
    "                norms.view(self.num_relations, 1).expand_as(self.relation_embeddings.weight))\n",
    "\n",
    "            self.relation_embeddings.weight.data[0] = torch.zeros(1, self.embedding_dim)\n",
    "            self.entity_embeddings.weight.data[0] = torch.zeros(1, self.embedding_dim)  # zeroing the padding index\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError(\"Haven't wired in the mechanism to load weights yet fam\")\n",
    "\n",
    "        # Also init the GUTS with bacteria and tapeworms\n",
    "        self.gat1.initialize(), self.gat2.initialize()\n",
    "            \n",
    "    def predict(self, triples_hops) -> torch.Tensor:\n",
    "        pass\n",
    "    \n",
    "    def normalize(self) -> None:\n",
    "        # Normalize embeddings of entities\n",
    "        norms = torch.norm(self.entity_embeddings.weight, p=self.l_p_norm_entities, dim=1).data\n",
    "        \n",
    "        self.entity_embeddings.weight.data = self.entity_embeddings.weight.data.div(\n",
    "            norms.view(self.num_entities, 1).expand_as(self.entity_embeddings.weight))\n",
    "        \n",
    "        # zeroing the padding index            \n",
    "        self.entity_embeddings.weight.data[0] = torch.zeros(1, self.embedding_dim)  \n",
    "            \n",
    "    def forward(self, pos: List, neg: List) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "            triples of size: (bs, 3)\n",
    "               hop1 of size: (bs, n, 2) (s and r)\n",
    "               hop2 of size: (bs, n, 3) (s and r1 and r2)\n",
    "\n",
    "            (here n -> num_neighbors)\n",
    "            (here hop2 has for bc it is <s r1 r2 o> )\n",
    "            \n",
    "            (pos has pos_triples, pos_hop1, pos_hop2. neg has same.)\n",
    "        \"\"\"\n",
    "        pos_triples, pos_hop1, pos_hop2 = pos\n",
    "        neg_triples, neg_hop1, neg_hop2 = neg\n",
    "\n",
    "        self.normalize()\n",
    "\n",
    "        positive_scores = self._score_triples(pos_triples, pos_hop1, pos_hop2)\n",
    "        negative_scores = self._score_triples(neg_triples, neg_hop1, neg_hop2)\n",
    "\n",
    "        loss = self._compute_loss(positive_scores=positive_scores, negative_scores=negative_scores)\n",
    "        return (positive_scores, negative_scores), loss\n",
    "\n",
    "\n",
    "    def _score_triples(self, \n",
    "                       triples: torch.Tensor, \n",
    "                       hop1: torch.Tensor, \n",
    "                       hop2: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" \n",
    "            triples of size: (bs, 3) \n",
    "            hop1 of size: (bs, n, 2)\n",
    "            hop2 of size: (bs, n, 3) \n",
    "            \n",
    "            1. Embed all things so triples (bs, 3, emb), hop1 (bs, n, 3, emb), hop2 (bs, n, 4, emb)\n",
    "            2. Concat hop1, hop2 to be (bs, n, 3*emb) and (bs, n, 4*emb) each\n",
    "            3. Pass the baton to some other function.\n",
    "        \"\"\"\n",
    "        triples, hop1, hop2 = self.embed(triples, hop1, hop2)\n",
    "        \n",
    "        # TODO: Check this view thing\n",
    "        hop1 = hop1.view(hop1.shape[0], hop1[1].shape[1], -1)\n",
    "        hop2 = hop2.view(hop2.shape[0], hop2[1].shape[1], -1)\n",
    "        \n",
    "        # DO SOMETHING\n",
    "        ....\n",
    "\n",
    "\n",
    "    def embed(self, tr, h1, h2):\n",
    "        \"\"\" The obj is to pass things through entity and rel matrices as needed \"\"\"\n",
    "        # Triple \n",
    "        s, p, o = slice_triples(tr, 3)                                  #*   : (bs, 1)\n",
    "        \n",
    "        \n",
    "        # Hop1\n",
    "        h1_s, h1_p = h1[:,:,0], h1[:,:,1]                               #h1_*: (bs, n, 1)\n",
    "        h1_o = triple[:,-1].repeat(1,h1.shape[1],1)                     #h1_o: (bs, n, emb)\n",
    "        h1_s = self.entity_embeddings(h1_s)                             #h1_s: (bs, n, emb)\n",
    "        h1_p = self.relation_embeddings(h1_p)                           #h1_p: (bs, n, emb)\n",
    "        \n",
    "        h1 = torch.cat((h1_s, h1_p, h1_o), dim=-1)                      #h1  : (bs, n, 3*emb)\n",
    "        \n",
    "        # Compute Mask\n",
    "        mask = compute_mask(h1)                                         #m   : (bs, n, 3*emb)\n",
    "        \n",
    "        gat1_op = self.gat1(h1, mask)                                   #op  : (bs, num_head*out_dim)\n",
    "        \n",
    "        \n",
    "         \n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def _get_relation_embeddings(self, relations):\n",
    "        return self.relation_embeddings(relations).view(-1, self.embedding_dim)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T15:23:52.459212Z",
     "start_time": "2019-10-02T15:23:52.454864Z"
    }
   },
   "outputs": [],
   "source": [
    "h1 = torch.randint(0, 10, (2, 4, 3))\n",
    "h1_s, h1_p, h1_o = h1[:,:,0], h1[:,:,1], h1[:,:,2]\n",
    "\n",
    "h1_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T15:23:52.587314Z",
     "start_time": "2019-10-02T15:23:52.581593Z"
    }
   },
   "outputs": [],
   "source": [
    "emb =  nn.Embedding(30, 5)\n",
    "h1_s, h1_p, h1_o = emb(h1_s), emb(h1_p), emb(h1_o)\n",
    "\n",
    "# h1_s.shape, h1_p.shape, h1_o.shape\n",
    "torch.cat((h1_s, h1_p, h1_s), dim=-1).shape\n",
    "# torch.cat((_a, _b, _c), dim=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T15:26:19.845572Z",
     "start_time": "2019-10-02T15:26:19.841567Z"
    }
   },
   "outputs": [],
   "source": [
    "help(compute_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#         tr_s, tr_p, tr_o = slice_triples(triples, slices = 3)      #each: (bs, 1)\n",
    "#         tr_s = self.entity_embeddings(tr_s)\n",
    "#         tr_p = self.relation_embeddings(tr_p)\n",
    "#         tr_o = self.entity_embeddings(tr_o)\n",
    "        \n",
    "#         tr = torch.cat((tr_s, tr_p, tr_o), dim=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
