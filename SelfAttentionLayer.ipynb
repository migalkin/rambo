{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple self attention layer with masking and support for multi headed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T12:27:56.833844Z",
     "start_time": "2019-10-07T12:27:55.210964Z"
    }
   },
   "outputs": [],
   "source": [
    "# Torch imports\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.autograd\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from typing import List, Optional, Dict, Tuple\n",
    "\n",
    "# Local imports\n",
    "from utils import *\n",
    "\n",
    "\n",
    "from models import slice_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T12:28:01.735557Z",
     "start_time": "2019-10-07T12:28:01.732201Z"
    }
   },
   "outputs": [],
   "source": [
    "bs = 4\n",
    "n = 5\n",
    "ent_emb_dims = 3\n",
    "rel_emb_dims = 4\n",
    "out_features = 7\n",
    "alpha_leaky = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T09:36:10.872008Z",
     "start_time": "2019-10-02T09:36:10.865535Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def self_attention_template():\n",
    "    # Setting things up\n",
    "    bs = 4\n",
    "    n = 5\n",
    "    ent_emb_dims = 3\n",
    "    rel_emb_dims = 4\n",
    "    out_features = 7\n",
    "    alpha_leaky = 0.2\n",
    "\n",
    "    matrix = torch.randn(bs,n,2*ent_emb_dims + rel_emb_dims) # concat s,p,o.\n",
    "    print(f\"shape of matrix is bs*n*emb_dim i.e {matrix.shape}\")\n",
    "    \n",
    "    # passing it through layer1\n",
    "    w1 = nn.Linear(2 * ent_emb_dim + rel_emb_dim, out_features)\n",
    "    nn.init.xavier_normal_(w1.weight.data, gain=1.414)\n",
    "\n",
    "    c = w1(matrix)\n",
    "    print(f\"shape of c is {c.shape}\")\n",
    "    \n",
    "    # passing it through layer2\n",
    "    w2 = nn.Linear(out_features,1)\n",
    "    nn.init.xavier_normal_(w2.weight.data, gain=1.414)\n",
    "\n",
    "    b = w2(c)\n",
    "    leaky_relu = nn.LeakyReLU(alpha_leaky)\n",
    "    b = leaky_relu(b).squeeze()\n",
    "    print(f\"shape of b is {b.shape}\")\n",
    "    \n",
    "    # There will be no masking here. So simply a softmax and then multiply and sum across n.\n",
    "    alphas = torch.softmax(b,dim=1)\n",
    "    h = torch.sum((alphas.unsqueeze(-1)*c),dim=1)\n",
    "    \n",
    "    print(f\"shape of final vector by {h.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T09:36:10.953642Z",
     "start_time": "2019-10-02T09:36:10.874775Z"
    }
   },
   "outputs": [],
   "source": [
    "self_attention_template()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T09:36:34.923973Z",
     "start_time": "2019-10-02T09:36:34.916228Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def self_attention_template_multi_head(num_head, final_layer=False):\n",
    "    # Setting things up\n",
    "    bs = 4\n",
    "    n = 5\n",
    "    ent_emb_dims = 3\n",
    "    rel_emb_dims = 4\n",
    "    out_features = 7\n",
    "    alpha_leaky = 0.2\n",
    "\n",
    "    matrix = torch.randn(bs,n,2*ent_emb_dims + rel_emb_dims) # concat s,p,o.\n",
    "    print(f\"shape of matrix is bs*n*emb_dim i.e {matrix.shape}\")\n",
    "    \n",
    "    # passing it through layer1\n",
    "    w1 = nn.Linear(2 * ent_emb_dims + rel_emb_dims, out_features)\n",
    "    nn.init.xavier_normal_(w1.weight.data, gain=1.414)\n",
    "\n",
    "    c = w1(matrix)\n",
    "    print(f\"shape of c is {c.shape}\")\n",
    "    \n",
    "    # passing it through layer2\n",
    "    w2 = nn.Linear(out_features,num_head)\n",
    "    nn.init.xavier_normal_(w2.weight.data, gain=1.414)\n",
    "\n",
    "    b = w2(c)\n",
    "    leaky_relu = nn.LeakyReLU(alpha_leaky)\n",
    "    b = leaky_relu(b).squeeze()\n",
    "    \n",
    "    print(f\"shape of b is {b.shape}\")\n",
    "    \n",
    "    # There will be no masking here. So simply a softmax and then multiply and sum across n.\n",
    "    alphas = torch.softmax(b,dim=1)\n",
    "    print(f\"shape of alphas is {alphas.shape}\")\n",
    "    \n",
    "    h = torch.bmm(c.transpose(1,2),alphas)\n",
    "    print(f\"shape of h is {h.shape}\")\n",
    "    if not final_layer:\n",
    "        h = h.view(bs,-1)\n",
    "        h = F.elu(h)\n",
    "    else:\n",
    "        h = torch.mean(h, dim=-1)\n",
    "        \n",
    "    print(f\"shape of final vector by {h.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T09:38:26.711467Z",
     "start_time": "2019-10-02T09:38:26.702955Z"
    }
   },
   "outputs": [],
   "source": [
    "self_attention_template_multi_head(num_head=8, final_layer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T12:28:08.241568Z",
     "start_time": "2019-10-07T12:28:08.229790Z"
    },
    "code_folding": [
     23
    ]
   },
   "outputs": [],
   "source": [
    "class GraphAttentionLayerMultihead(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: dict, final_layer: bool = False):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Parse params\n",
    "        ent_emb_dim, rel_emb_dim = config['EMBEDDING_DIM'], config['EMBEDDING_DIM']\n",
    "        out_features = config['KBGATARGS']['OUT']\n",
    "        num_head = config['KBGATARGS']['HEAD']\n",
    "        alpha_leaky = config['KBGATARGS']['ALPHA']\n",
    "        \n",
    "        self.w1 = nn.Linear(2 * ent_emb_dim + rel_emb_dim, out_features)\n",
    "        self.w2 = nn.Linear(out_features, num_head)\n",
    "        self.relu = nn.LeakyReLU(alpha_leaky)\n",
    "\n",
    "        self.final = final_layer\n",
    "        \n",
    "        # Why copy un-necessary stuff\n",
    "        self.heads = num_head\n",
    "        \n",
    "        # Not initializing here. Should be called by main module\n",
    "    \n",
    "    def initialize(self):\n",
    "        nn.init.xavier_normal_(self.w1.weight.data, gain=1.414)\n",
    "        nn.init.xavier_normal_(self.w2.weight.data, gain=1.414)\n",
    "        \n",
    "    def forward(self, data: torch.Tensor, mask: torch.Tensor=None):\n",
    "        \"\"\" \n",
    "            data: size (batchsize, num_neighbors, 2*ent_emb+rel_emb) or (bs, n, emb)\n",
    "            mask: size (batchsize, num_neighbors)\n",
    "            \n",
    "            PS: num_neighbors is padded either with max neighbors or with a limit \n",
    "        \"\"\"\n",
    "        \n",
    "                                                      #data: bs, n, emb\n",
    "        c = self.w1(data)                                #c: bs, n, out_features\n",
    "        b = self.relu(self.w2(c)).squeeze()              #b: bs, n, num_heads\n",
    "        m = mask.unsqueeze(-1).repeat(1, 1, self.heads)  #m: bs, n, num_heads\n",
    "        alphas = masked_softmax(b, m, dim=1)             #Î±: bs, n, num_heads\n",
    "        \n",
    "#         print(alphas)\n",
    "#         print(mask)\n",
    "        \n",
    "        # BMM simultaneously weighs the triples and sums across neighbors\n",
    "        h = torch.bmm(c.transpose(1,2),alphas)          #h: bs, out_features, num_heads\n",
    "        \n",
    "        if self.final:\n",
    "            h = torch.mean(h, dim=-1)                   #h: bs, out_features\n",
    "        else:\n",
    "            h = F.elu(h.view(bs, -1))                   #h: bs, out_features*num_heads\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T12:28:11.281674Z",
     "start_time": "2019-10-07T12:28:11.275742Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    bs = 4\n",
    "    n = 5\n",
    "    ent_emb_dims = 3\n",
    "    rel_emb_dims = 4\n",
    "    out_features = 7\n",
    "    alpha_leaky = 0.2\n",
    "\n",
    "    attn = GraphAttentionLayerMultihead(ent_emb_dims, rel_emb_dims, \n",
    "                                        out_features, alpha_leaky, num_head=8, final_layer=False)\n",
    "    print(attn)\n",
    "\n",
    "    data = torch.randn(bs, n, 2*ent_emb_dims+rel_emb_dims)\n",
    "    data[0][2:] = 0\n",
    "    data[1][4:] = 0\n",
    "    data[-1][1:] = 0\n",
    "\n",
    "    mask = compute_mask(data)\n",
    "    mask_condensed = torch.mean(mask, dim=-1)\n",
    "\n",
    "    print(data.shape)\n",
    "    op = attn(data, mask_condensed)\n",
    "\n",
    "    op, op.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Model Shit\n",
    "\n",
    "# init\n",
    "\n",
    "\"\"\"\n",
    "    -> init transE thing.\n",
    "    -> load pretrained weights\n",
    "    -> init the GAT layers (2)\n",
    "    \n",
    "    -> initialize nicely everything\n",
    "\"\"\"\n",
    "\n",
    "# forward\n",
    "\"\"\"\n",
    "    data: (bs, 3) :triple; (bs, neighborhood, 3) for Os; (bs, neighborhood, 3) for Ss'.... (also for hop2)\n",
    "    gat1(neighborhoodO)\n",
    "    gat2(neighborhoodO)\n",
    "    \n",
    "    # Same for S?\n",
    "    \n",
    "    embed bs, 1 -> relations\n",
    "    \n",
    "    transE loss.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T11:50:27.538962Z",
     "start_time": "2019-10-02T11:50:27.534836Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class KBGat(BaseModule):\n",
    "    \n",
    "    model_name = 'KBGAT'\n",
    "    \n",
    "    def __init__(self, config: dict, pretrained_embeddings=None) -> None:\n",
    "        \n",
    "        self.margin_ranking_loss_size_average: bool = True\n",
    "        self.entity_embedding_max_norm: Optional[int] = None\n",
    "        self.entity_embedding_norm_type: int = 2\n",
    "        self.model_name = 'KBGAT'\n",
    "        super().__init__(config)\n",
    "        self.statement_len = config['STATEMENT_LEN']\n",
    "\n",
    "        # Embeddings\n",
    "        self.l_p_norm_entities = config['NORM_FOR_NORMALIZATION_OF_ENTITIES']\n",
    "        self.scoring_fct_norm = config['SCORING_FUNCTION_NORM']\n",
    "        self.relation_embeddings = nn.Embedding(config['NUM_RELATIONS'], config['EMBEDDING_DIM'], padding_idx=0)\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        if self.config['PROJECT_QUALIFIERS']:\n",
    "            self.proj_mat = nn.Linear(2*self.embedding_dim, self.embedding_dim, bias=False)\n",
    "            \n",
    "        self.gat1 = GraphAttentionLayerMultihead(self.config, final_layer=False)\n",
    "        self.gat2 = GraphAttentionLayerMultihead(self.config, final_layer=True)\n",
    "        \n",
    "        # Note: not initing them\n",
    "        self.wr = nn.Linear(config['EMBEDDING_DIM'], config['KBGATARGS']['OUT'])\n",
    "        self.we = nn.Linear(config['EMBEDDING_DIM'], config['KBGATARGS']['OUT'])\n",
    "        \n",
    "        # Put in weights\n",
    "        self._initialize(pretrained_embeddings)\n",
    "        \n",
    "    def _initialize(self, pretrained_embeddings):\n",
    "        if pretrained_embeddings is None:\n",
    "            embeddings_init_bound = 6 / np.sqrt(self.config['EMBEDDING_DIM'])\n",
    "            nn.init.uniform_(\n",
    "                self.entity_embeddings.weight.data,\n",
    "                a=-embeddings_init_bound,\n",
    "                b=+embeddings_init_bound,\n",
    "            )\n",
    "            nn.init.uniform_(\n",
    "                self.relation_embeddings.weight.data,\n",
    "                a=-embeddings_init_bound,\n",
    "                b=+embeddings_init_bound,\n",
    "            )\n",
    "\n",
    "            norms = torch.norm(self.relation_embeddings.weight,\n",
    "                               p=self.config['NORM_FOR_NORMALIZATION_OF_RELATIONS'], dim=1).data\n",
    "            self.relation_embeddings.weight.data = self.relation_embeddings.weight.data.div(\n",
    "                norms.view(self.num_relations, 1).expand_as(self.relation_embeddings.weight))\n",
    "\n",
    "            self.relation_embeddings.weight.data[0] = torch.zeros(1, self.embedding_dim)\n",
    "            self.entity_embeddings.weight.data[0] = torch.zeros(1, self.embedding_dim)  # zeroing the padding index\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError(\"Haven't wired in the mechanism to load weights yet fam\")\n",
    "\n",
    "        # Also init the GATs with bacteria and tapeworms\n",
    "        self.gat1.initialize(), self.gat2.initialize()\n",
    "            \n",
    "    def predict(self, triples_hops) -> torch.Tensor:\n",
    "        pass\n",
    "    \n",
    "    def normalize(self) -> None:\n",
    "        # Normalize embeddings of entities\n",
    "        norms = torch.norm(self.entity_embeddings.weight, p=self.l_p_norm_entities, dim=1).data\n",
    "        \n",
    "        self.entity_embeddings.weight.data = self.entity_embeddings.weight.data.div(\n",
    "            norms.view(self.num_entities, 1).expand_as(self.entity_embeddings.weight))\n",
    "        \n",
    "        # zeroing the padding index            \n",
    "        self.entity_embeddings.weight.data[0] = torch.zeros(1, self.embedding_dim)  \n",
    "            \n",
    "    def forward(self, pos: List, neg: List) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "            triples of size: (bs, 3)\n",
    "               hop1 of size: (bs, n, 2) (s and r)\n",
    "               hop2 of size: (bs, n, 3) (s and r1 and r2)\n",
    "\n",
    "            (here n -> num_neighbors)\n",
    "            (here hop2 has for bc it is <s r1 r2 o> )\n",
    "            \n",
    "            (pos has pos_triples, pos_hop1, pos_hop2. neg has same.)\n",
    "        \"\"\"\n",
    "        pos_triples, pos_hop1, pos_hop2 = pos\n",
    "        neg_triples, neg_hop1, neg_hop2 = neg\n",
    "\n",
    "        self.normalize()\n",
    "\n",
    "        positive_scores = self._score_triples(pos_triples, pos_hop1, pos_hop2)\n",
    "        negative_scores = self._score_triples(neg_triples, neg_hop1, neg_hop2)\n",
    "\n",
    "        loss = self._compute_loss(positive_scores=positive_scores, negative_scores=negative_scores)\n",
    "        return (positive_scores, negative_scores), loss\n",
    "\n",
    "\n",
    "    def score_triples(self, \n",
    "                       triples: torch.Tensor, \n",
    "                       hop1: torch.Tensor, \n",
    "                       hop2: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" \n",
    "            triples of size: (bs, 3) \n",
    "            hop1 of size: (bs, n, 2) (s, p) (o is same as that of triples)\n",
    "            hop2 of size: (bs, n, 3) (s, p1, p2) (o is same as that of triples)\n",
    "            \n",
    "            1. Embed all things so triples (bs, 3, emb), hop1 (bs, n, 3, emb), hop2 (bs, n, 4, emb)\n",
    "            2. Concat hop1, hop2 to be (bs, n, 3*emb) and (bs, n, 4*emb) each\n",
    "            3. Pass the baton to some other function.\n",
    "        \"\"\"\n",
    "        s, p, o, h1_s, h1_p, h2_s, h2_p1, h2_p2 = self.embed(triples, hop1, hop2)\n",
    "        \n",
    "        hf = self._score_o(s, p, o, h1_s, h1_p, h2_s, h2_p1, h2_p2)\n",
    "        sum_res = s + p - hf\n",
    "        distances = torch.norm(sum_res, dim=1, p=self.scoring_fct_norm).view(size=(-1,))\n",
    "        return distances        \n",
    "    \n",
    "    def _score_o(self, s: torch.Tensor, p: torch.Tensor, o: torch.Tensor,\n",
    "                 h1_s: torch.Tensor, h1_p: torch.Tensor,\n",
    "                 h2_s: torch.Tensor, h2_p1: torch.Tensor, h2_p2: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "            Expected embedded tensors: following\n",
    "            \n",
    "            s:  (bs, emb)\n",
    "            p:  (bs, emb)\n",
    "            o:  (bs, emb)\n",
    "            h1_s: (bs, n, emb)\n",
    "            h1_p: (bs, n, emb)\n",
    "            h2_s: (bs, n, emb)\n",
    "            h2_p1: (bs, n, emb)\n",
    "            h2_p2: (bs, n, emb)\n",
    "            \n",
    "            Next:\n",
    "              -> compute mask1, cat o to it, and push to gat 1\n",
    "              -> compute mask2, cat gat1op to it, and push to gat 2\n",
    "              -> do all the residual connections\n",
    "              -> return final score\n",
    "        \"\"\"\n",
    "        \n",
    "        # Compute Masks\n",
    "        mask1 =  compute_mask(h1_s)[:,:,0]                              #m1   : (bs, n)\n",
    "        mask2 =  compute_mask(h2_s)[:,:,0]                              #m2   : (bs, n)\n",
    "        \n",
    "        # Cat `o` in in h1\n",
    "        h1_o = o.repeat(1,h1_s.shape[1],1)                              #h1_o : (bs, n, emb)\n",
    "        h1_o = h1_o*mask1.unsqueeze(-1)                                 #h1_o : (bs, n, emb)\n",
    "        h1 = torch.cat((h1_s, h1_p, h1_o), dim=-1)                      #h1   : (bs, n, 3*emb)\n",
    "        \n",
    "        # Pass to first graph attn layer\n",
    "        gat1_op = self.gat1(h1, mask1)                                  #op   : (bs, num_head*out_dim)\n",
    "        \n",
    "        # Do the G` = G*W thing here\n",
    "        gat1_p = self.wr(p)                                             #rels : (bs, emb')\n",
    "        gat1_op_concat = torch.cat((gat1_op, gat1_p), dim=-1)           #op   : (bs, emb'+num_head*out_dim)\n",
    "        \n",
    "        # Average h2_p1, h2_p2\n",
    "        h2_p = (h2_p1 + h2_p2)/2.0                                      #h2_p : (bs, n, emb)\n",
    "        \n",
    "        # Treat this as the new \"o\", and throw in h2 data as well.\n",
    "        h2_o = gat1_op_concat.unsqueeze(1).repeat(1, h2.shape[1], 1)    #h2_o : (bs, n, num_head*out_dim + emb')\n",
    "        h2_o = h2_o*mask2.unsqueeze(-1)                                 #h2_o : (bs, n, num_head*out_dim + emb')\n",
    "        h2 = torch.cat((h2_s, h2_p, h2_o), dim=-1)                      #h2   : (bs, n, 2*emb + num_head*out_dim + emb')\n",
    "        \n",
    "        # Pass to second graph attn layer\n",
    "        hf = self.gat2(h2, mask2)                                       #hf   : (bs, out_dim)\n",
    "        \n",
    "        # Eq. 12 (H'' = W^EH^T + H^F)\n",
    "        hf = torch.cat((hf, self.we(o)))                                #hf   : (bs, out_dim*2)\n",
    "        \n",
    "        return hf\n",
    "\n",
    "    def embed(self, tr, h1, h2):\n",
    "        \"\"\" The obj is to pass things through entity and rel matrices as needed \"\"\"\n",
    "        # Triple \n",
    "        s, p, o = slice_triples(tr, 3)                                  #*    : (bs, 1)\n",
    "        \n",
    "        s = self.entity_embeddings(s).unsqueeze(1)\n",
    "        p = self.relation_embeddings(p).unsqueeze(1)\n",
    "        o = self.entity_embeddings(o).unsqueeze(1)                      #o    : (bs, 1, emb)\n",
    "\n",
    "        # Hop1\n",
    "        h1_s, h1_p = h1[:,:,0], h1[:,:,1]                               #h1_* : (bs, n)\n",
    "        \n",
    "        h1_s = self.entity_embeddings(h1_s)                             #h1_s : (bs, n, emb)\n",
    "        h1_p = self.relation_embeddings(h1_p)                           #h1_p : (bs, n, emb)\n",
    "        \n",
    "        # Hop2\n",
    "        h2_s, h2_p1, h2_p2 = h2[:, :, 0], h2[:, :, 1], h2[:, :, 2]      #h2_* : (bs, n)\n",
    "        \n",
    "        h2_s = self.entity_embeddings(h2_s)                             #h2_s : (bs, n, emb)\n",
    "        h2_p1 = self.relation_embeddings(h2_p1)                         #h2_p1: (bs, n, emb)\n",
    "        h2_p2 = self.relation_embeddings(h2_p2)                         #h2_p2: (bs, n, emb)\n",
    "        \n",
    "        return s, p, o, h1_s, h1_p, h2_s, h2_p1, h2_p2\n",
    "        \n",
    "\n",
    "    def _get_relation_embeddings(self, relations):\n",
    "        return self.relation_embeddings(relations).view(-1, self.embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T12:35:41.293422Z",
     "start_time": "2019-10-07T12:35:41.275130Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    h2 = torch.randn(2, 4, 3)\n",
    "    h2_a, h2_b, h2_c = h2[:,:,0], h2[:,:,1], h2[:,:,2]\n",
    "\n",
    "    h2.shape, h2_a.shape\n",
    "\n",
    "    h1 = torch.randint(0, 10, (2, 4, 3))\n",
    "    h1_s, h1_p, h1_o = h1[:,:,0], h1[:,:,1], h1[:,:,2]\n",
    "\n",
    "    h1_s.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T12:46:25.712853Z",
     "start_time": "2019-10-07T12:46:25.700648Z"
    }
   },
   "source": [
    "s, p, o = torch.randn(2, 5), torch.randn(2, 5), torch.randn(2, 5)\n",
    "\n",
    "torch.cat((s, p, o), dim=1).shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T15:23:52.587314Z",
     "start_time": "2019-10-02T15:23:52.581593Z"
    }
   },
   "source": [
    "emb =  nn.Embedding(30, 5)\n",
    "h1_s, h1_p, h1_o = emb(h1_s), emb(h1_p), emb(h1_o)\n",
    "\n",
    "# h1_s.shape, h1_p.shape, h1_o.shape\n",
    "torch.cat((h1_s, h1_p, h1_s), dim=-1).shape\n",
    "# torch.cat((_a, _b, _c), dim=-1).shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T15:50:44.274941Z",
     "start_time": "2019-10-02T15:50:44.268810Z"
    }
   },
   "source": [
    "s, p, o = torch.randn(2, 5).unsqueeze(1), torch.randn(2, 5).unsqueeze(1), torch.randn(2, 5).unsqueeze(1)\n",
    "\n",
    "torch.cat((s, p, o), dim=1).shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#         tr_s, tr_p, tr_o = slice_triples(triples, slices = 3)      #each: (bs, 1)\n",
    "#         tr_s = self.entity_embeddings(tr_s)\n",
    "#         tr_p = self.relation_embeddings(tr_p)\n",
    "#         tr_o = self.entity_embeddings(tr_o)\n",
    "        \n",
    "#         tr = torch.cat((tr_s, tr_p, tr_o), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T14:33:14.398379Z",
     "start_time": "2019-10-07T14:33:14.390694Z"
    }
   },
   "outputs": [],
   "source": [
    "# Managing masks\n",
    "h1 = torch.randint(1, 10, (bs, n, ent_emb_dims+rel_emb_dims))\n",
    "h1[1][2:] = 0\n",
    "h1[0][1:] = 0\n",
    "h1[2][4:] = 0\n",
    "h1[3][5:] = 0\n",
    "\n",
    "mask =  compute_mask(h1)[:,:,0]\n",
    "\n",
    "h1, mask\n",
    "\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T14:34:03.728872Z",
     "start_time": "2019-10-07T14:34:03.720889Z"
    }
   },
   "outputs": [],
   "source": [
    "o = torch.randint(1, 10, (bs, ent_emb_dims), dtype=torch.float)\n",
    "o = o.unsqueeze(1).repeat(1, n, 1)\n",
    "o # bs, n, embdim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T14:34:04.830700Z",
     "start_time": "2019-10-07T14:34:04.826275Z"
    }
   },
   "outputs": [],
   "source": [
    "o.shape, mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T14:54:11.532573Z",
     "start_time": "2019-10-07T14:54:11.525180Z"
    }
   },
   "outputs": [],
   "source": [
    "h2_p1, h2_p2 = torch.randn(bs,n,rel_emb_dims), torch.randn(bs,n,rel_emb_dims)\n",
    "(h2_p1+h2_p2)/2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:23:39.894003Z",
     "start_time": "2019-10-07T13:23:39.886500Z"
    }
   },
   "outputs": [],
   "source": [
    "pr = torch.randint(1, 10, (bs, n, ent_emb_dims+rel_emb_dims))\n",
    "pr, o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T13:23:42.621709Z",
     "start_time": "2019-10-07T13:23:42.615882Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cat((pr, o), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T14:45:20.047490Z",
     "start_time": "2019-10-07T14:45:20.040918Z"
    }
   },
   "outputs": [],
   "source": [
    "h1_p = torch.randn(bs, n, rel_emb_dims+2)\n",
    "gat1op = torch.randn(bs, 11)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
